{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import h5py\n",
    "import RESNET152_ATT_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmat(filename):\n",
    "    '''\n",
    "    读取 MATLAB v7.3 `.mat` 文件（Whole_tracks 作为 tracks）\n",
    "    '''\n",
    "    output = dict()\n",
    "    \n",
    "    # 打开 HDF5 MAT 文件\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        # 读取 Whole_tracks 变量\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'Whole_tracks' 变量不存在！\")\n",
    "\n",
    "        whole_tracks = data['Whole_tracks']  # 结构体 Whole_tracks\n",
    "\n",
    "        # 确保它有 `count` 和 `data`\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"❌ 错误: 'Whole_tracks' 结构不完整！包含: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # 读取 count（可能是字符编码格式，需要解析）\n",
    "        count = whole_tracks['count'][()]  \n",
    "        print(\"🔍 Whole_tracks['count'] 数据:\", count)\n",
    "        print(\"🔍 数据类型:\", type(count))\n",
    "\n",
    "        # 直接转换成整数\n",
    "        total_count = int(count.item())\n",
    "        print(f'total_count: {total_count}')\n",
    "        # 读取 Whole_tracks['data']\n",
    "        track = []\n",
    "        for i in range(total_count):\n",
    "            data_ref = whole_tracks['data'][i].item()\n",
    "            track.append(np.transpose(data[data_ref][:]).astype(np.float32))\n",
    "\n",
    "        # 组织输出\n",
    "        output['tracks'] = {\n",
    "            'count': total_count,\n",
    "            'data': track\n",
    "        }\n",
    "    \n",
    "    return output\n",
    "\n",
    "def _check_keys(dict):\n",
    "    '''\n",
    "    checks if entries in dictionary are mat-objects. If yes\n",
    "    todict is called to change them to nested dictionaries\n",
    "    '''\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict        \n",
    "\n",
    "def _todict(matobj):\n",
    "    '''\n",
    "    A recursive function which constructs from matobjects nested dictionaries\n",
    "    '''\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict\n",
    "\n",
    "#%%\n",
    "def mySoftmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\"\"\"normalize\"\"\"#110\n",
    "def rescale(X_list,count):\n",
    "    output=list()\n",
    "    if count==1:\n",
    "        output.append(X_list/110)\n",
    "        return output\n",
    "    for i in range(len(X_list)):\n",
    "        output.append(X_list[i]/110)\n",
    "    return output\n",
    "\n",
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "def datato3d(arrays):#list of np arrays, NULL*3*100\n",
    "    output=list()\n",
    "    for i in arrays:\n",
    "        i=np.squeeze(i,axis=1)\n",
    "        i=np.transpose(i,(0,2,1))\n",
    "        output.append(i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 处理数据: ../Testing_Set/J0037_tracks.mat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.utils.data as utils\n",
    "\n",
    "# 参数\n",
    "matpath = '../Testing_Set/J0037_tracks.mat'\n",
    "label_path = '../Testing_Set/J0037_class_label.mat'  # 你的标签文件\n",
    "classnum = 15  # 类别数\n",
    "ROI_EMBEDDING_DIM = 32\n",
    "\n",
    "def loadmat(filename):\n",
    "    \"\"\" 读取 MATLAB v7.3 .mat 文件 \"\"\"\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'Whole_tracks' 变量不存在！\")\n",
    "        \n",
    "        whole_tracks = data['Whole_tracks']\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"❌ 错误: 'Whole_tracks' 结构不完整！包含: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # 读取 count\n",
    "        count = int(whole_tracks['count'][()].item())\n",
    "        track = [np.transpose(data[whole_tracks['data'][i].item()][:]).astype(np.float32) for i in range(count)]\n",
    "    \n",
    "    return {'tracks': {'count': count, 'data': track}}\n",
    "\n",
    "def load_labels(label_path):\n",
    "    \"\"\" 读取标签 .mat 文件 \"\"\"\n",
    "    with h5py.File(label_path, 'r') as data:\n",
    "        if 'class_label' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'class_label' 变量不存在！\")\n",
    "        \n",
    "        class_label = data['class_label'][()]\n",
    "        \n",
    "        if isinstance(class_label, np.ndarray):\n",
    "            if class_label.size == 1:  \n",
    "                class_label = class_label.item()\n",
    "            else:  \n",
    "                class_label = np.array(class_label)\n",
    "        else:\n",
    "            class_label = int(class_label)\n",
    "\n",
    "        print(f\"✅ 成功解析 class_label, 形状: {class_label.shape}\")\n",
    "        return class_label\n",
    "\n",
    "\n",
    "\"\"\" 测试模型 \"\"\"\n",
    "args_test_batch_size = 10000\n",
    "NCLASS = int(classnum)\n",
    "\n",
    "print(f\"📌 处理数据: {matpath}\")\n",
    "mat = loadmat(matpath)\n",
    "X_test = mat['tracks']['data']\n",
    "X_test = np.asarray(X_test).astype(np.float32)\n",
    "X_test_original = np.transpose(X_test, (0, 2, 1))  # 维度转换\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功解析 class_label, 形状: (234541, 1)\n",
      "(234541, 4, 100)\n",
      "(234541, 1)\n",
      "(469082, 4, 100)\n",
      "(469082,)\n"
     ]
    }
   ],
   "source": [
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "    y_nparray = y_nparray.flatten()\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "# 读取标签\n",
    "y_test = load_labels(label_path)\n",
    "y_test_list = y_test\n",
    "print(X_test_original.shape)\n",
    "print(y_test.shape)\n",
    "X_test, y_test = udflip(X_test_original,y_test,shuffle=False)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "y_test = torch.from_numpy(y_test.astype(np.int64))  # 确保是整数类型\n",
    "X_test = torch.from_numpy(X_test)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "tst_set = utils.TensorDataset(X_test, y_test)\n",
    "tst_loader = utils.DataLoader(tst_set, batch_size=args_test_batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469082"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([469082, 4, 100]), torch.Size([469082]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_at_test(probs,mode='max'):\n",
    "    assert(len(probs)>0)\n",
    "    if(mode=='max'):\n",
    "        all_probs=np.vstack(probs)\n",
    "        print(all_probs.shape)\n",
    "        max_probs=np.amax(all_probs,axis=1).reshape((2,-1))#row 0: prob for first half, row 1: prob for flipped half\n",
    "        max_idx=np.argmax(max_probs,axis=0)#should be 0/1\n",
    "        test_sample_count=all_probs.shape[0]/2\n",
    "        \n",
    "        class_pred=np.argmax(all_probs,axis=1)\n",
    "        final_pred=list()\n",
    "        for i in range(max_idx.shape[0]):\n",
    "            final_pred.append(class_pred[int(i+test_sample_count*max_idx[i])])#if 0, first half\n",
    "        return final_pred\n",
    "    if(mode=='mean'):\n",
    "        all_probs=np.exp(np.vstack(probs))\n",
    "        test_sample_count=int(all_probs.shape[0]/2)\n",
    "        final_probs=all_probs[0:test_sample_count]+all_probs[test_sample_count:]\n",
    "        final_pred=np.argmax(final_probs,axis=1)\n",
    "        return final_pred.tolist()\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def aug_at_test(probs, mode='max'):\n",
    "#     \"\"\"\n",
    "#     适用于 **无数据增强** 的版本：\n",
    "#     - 直接选择 `argmax` 作为最终预测\n",
    "#     - `mode='max'` 或 `mode='mean'` 影响不大，因为没有翻转数据\n",
    "    \n",
    "#     参数：\n",
    "#     - probs: 模型输出的 logits 列表，每个 batch 存储一次输出\n",
    "    \n",
    "#     返回：\n",
    "#     - final_pred: 预测类别列表\n",
    "#     \"\"\"\n",
    "#     assert len(probs) > 0, \"probs 为空，无法计算预测结果\"\n",
    "\n",
    "#     # 合并所有 batch\n",
    "#     all_probs = np.vstack(probs)  # 形状: (N, num_classes)\n",
    "\n",
    "#     # 直接取最大概率类别作为预测类别\n",
    "#     final_pred = np.argmax(all_probs, axis=1)\n",
    "\n",
    "#     return final_pred.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:1\n",
      "using ROI with emb: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bohan/.conda/envs/deterministic-a-bridge/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/bohan/projects/DCNN_Dice/RESNET152_ATT_naive.py:212: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x), embed, x_att, x, out1, out2, out3, final_feat, out1_feat, out2_feat, out3_feat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(469082, 15)\n",
      "\tCenter loss: 0.0000\n",
      "\tfocal loss: 0.0065\n",
      "Test set avg loss: 1185.4198\n",
      "\tClustering loss: 1185.4134\n",
      "Precision, Recall, macro F1: 0.9180087466607116 0.8928001436431507 0.9028530012430607\n"
     ]
    }
   ],
   "source": [
    "# 数据加载\n",
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "from Embedding_layer import ROIFeatureExtractor\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import RESNET152_ATT_naive\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "from Util import focalLoss, preprocess_fiber_input\n",
    "from clustering_layer_v2 import ClusterlingLayer\n",
    "from klDiv import KLDivLoss\n",
    "\n",
    "modelpath = 'save_small/focal_loss_and_cluster_loss_c_10.0_FE.model'\n",
    "# 加载模型\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "ROI_EMBEDDING_DIM = 32\n",
    "NUM_ROI_CLASSES = 726 + 1\n",
    "HIDDEN_DIM = 64\n",
    "model=RESNET152_ATT_naive.resnet18(num_classes=NCLASS, input_ch=3+ROI_EMBEDDING_DIM)\n",
    "# init ROI Embedding layer\n",
    "roi_embedding_layer = nn.Embedding(NUM_ROI_CLASSES, ROI_EMBEDDING_DIM).to(device)\n",
    "# init FE\n",
    "roi_extractor = ROIFeatureExtractor(roi_embedding_layer, ROI_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM).to(device)\n",
    "roi_extractor.to(device)\n",
    "model.to(device)\n",
    "\n",
    "# 2️⃣ 加载权重\n",
    "state_dict = torch.load(modelpath, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "log_testing_total_loss = 0.0\n",
    "log_focal_loss = 0.0\n",
    "log_centering_loss= 0.\n",
    "log_clustering_loss = 0.0\n",
    "probs = []\n",
    "preds = []\n",
    "labels = []\n",
    "clustering_layer = ClusterlingLayer(embedding_dimension=512, num_clusters=NCLASS, alpha=1.0)\n",
    "kl_loss = KLDivLoss(NCLASS, loss_weight=2.0, temperature=2)\n",
    "kl_loss.to(device)\n",
    "clustering_layer.to(device)\n",
    "global global_cluster_rois  # Ensure global access to cluster anatomical profiles\n",
    "loss_nll = nn.NLLLoss(size_average=True) # log-softmax applied in the network\n",
    "with torch.no_grad():\n",
    "    for data, target in tst_loader:\n",
    "        labels += target.cpu().numpy().tolist()\n",
    "        # if args.cuda:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # target = target.squeeze(1)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # print(data.shape)\n",
    "        # print(target.shape)\n",
    "        data_processed = preprocess_fiber_input(data, roi_extractor=roi_extractor, device=device, net_type='FE')\n",
    "        output, embed, _, _, _, _, _, _, _, _, _ = model(data_processed)\n",
    "\n",
    "        # Compute focal loss\n",
    "        floss = focalLoss(output, target, loss_nll=loss_nll)\n",
    "        total_loss = floss\n",
    "        log_focal_loss += floss.item()\n",
    "        # Compute center loss if enabled\n",
    "\n",
    "        # Compute clustering loss if enabled\n",
    "        clustering_out, x_dis = clustering_layer(embed)\n",
    "\n",
    "        # Get predicted cluster labels\n",
    "        tar_dist = ClusterlingLayer.create_soft_labels(target, NCLASS, temperature=2).to(target.device)\n",
    "        loss_clust = 10 * kl_loss.kl_div_cluster(torch.log(clustering_out), tar_dist) / 1024\n",
    "\n",
    "        total_loss += loss_clust\n",
    "        log_clustering_loss += loss_clust.item()\n",
    "\n",
    "        # Accumulate total test loss\n",
    "        log_testing_total_loss += total_loss.item()\n",
    "        probs.append(output.data.cpu().numpy())\n",
    "\n",
    "# Compute final predictions using test-time augmentation\n",
    "preds = aug_at_test(probs, mode='max')\n",
    "num_batch = len(tst_loader) / 1024\n",
    "\n",
    "# Compute evaluation metrics\n",
    "conf_mat = confusion_matrix(y_test_list, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_list, preds, average='macro')\n",
    "\n",
    "avg_testing_loss = log_testing_total_loss / num_batch\n",
    "avg_clustering_loss = log_clustering_loss / num_batch\n",
    "print('\\tCenter loss: {:.4f}'.format(log_centering_loss / num_batch))\n",
    "print('\\tfocal loss: {:.4f}'.format(log_focal_loss/num_batch))\n",
    "print(f'Test set avg loss: {avg_testing_loss:.4f}')\n",
    "\n",
    "print(f'\\tClustering loss: {avg_clustering_loss:.4f}')\n",
    "print('Precision, Recall, macro F1:', precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bohan/.conda/envs/deterministic-a-bridge/lib/python3.9/site-packages/ipykernel_launcher.py\n",
      "--f=/home/bohan/.local/share/jupyter/runtime/kernel-v3954b1db88fadc1a3a24ceac916b91b22e4d04f93.json\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sys\u001b[38;5;241m.\u001b[39margv))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# checkArgc(sys.argv)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmain\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(sys.argv))\n",
    "# checkArgc(sys.argv)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filename = \"../Testing_Set/J0037_tracks.mat\"\n",
    "\n",
    "# 读取 MAT v7.3 文件\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    print(\"MAT 文件中的变量:\", list(f.keys()))  # 输出所有变量名称\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename, 'r') as f:\n",
    "    if 'tracks' in f:\n",
    "        print(\"tracks 内部结构:\", list(f['tracks'].keys()))\n",
    "    else:\n",
    "        print(\"❌ 错误: 'tracks' 不存在！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename, 'r') as f:\n",
    "    print(\"MAT 文件中的变量:\", list(f.keys()))  # 顶层变量\n",
    "\n",
    "    # 检查 Whole_tracks 里面的内容\n",
    "    if 'Whole_tracks' in f:\n",
    "        print(\"\\n🔍 Whole_tracks 结构:\")\n",
    "        try:\n",
    "            print(\"    子变量:\", list(f['Whole_tracks'].keys()))  # 打印 Whole_tracks 内部结构\n",
    "        except AttributeError:\n",
    "            print(\"    Whole_tracks 不是结构体，可能是数组或标量\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def loadmat(filename):\n",
    "    '''\n",
    "    读取 MATLAB v7.3 `.mat` 文件（Whole_tracks 作为 tracks）\n",
    "    '''\n",
    "    output = dict()\n",
    "    \n",
    "    # 打开 HDF5 MAT 文件\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        # 读取 Whole_tracks 变量\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'Whole_tracks' 变量不存在！\")\n",
    "\n",
    "        whole_tracks = data['Whole_tracks']  # 结构体 Whole_tracks\n",
    "\n",
    "        # 确保它有 `count` 和 `data`\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"❌ 错误: 'Whole_tracks' 结构不完整！包含: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # 读取 count（可能是字符编码格式，需要解析）\n",
    "        count = whole_tracks['count'][()]  \n",
    "        print(\"🔍 Whole_tracks['count'] 数据:\", count)\n",
    "        print(\"🔍 数据类型:\", type(count))\n",
    "\n",
    "        # 直接转换成整数\n",
    "        total_count = int(count.item())\n",
    "        # 读取 Whole_tracks['data']\n",
    "        track = []\n",
    "        for i in range(total_count):\n",
    "            data_ref = whole_tracks['data'][i].item()\n",
    "            track.append(np.transpose(data[data_ref][:]).astype(np.float32))\n",
    "\n",
    "        # 组织输出\n",
    "        output['tracks'] = {\n",
    "            'count': total_count,\n",
    "            'data': track\n",
    "        }\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 运行测试\n",
    "filename = \"../Testing_Set/J0037_tracks.mat\"\n",
    "tracks_data = loadmat(filename)\n",
    "print(\"✅ 成功读取 Whole_tracks 数据！\")\n",
    "print(f\"轨迹数: {tracks_data['tracks']['count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=tracks_data['tracks']['data']\n",
    "X_test=rescale(X_test,int(tracks_data['tracks']['count']))\n",
    "#X_test_ud=np.asarray(udflip(X_test,int(mat['tracks']['count']))).astype(np.float32)\n",
    "X_test=np.asarray(X_test).astype(np.float32)\n",
    "#X_test=np.vstack((X_test,X_test_ud))\n",
    "#X_test=X_test.reshape((X_test.shape[0],1,X_test.shape[1],X_test.shape[2]))\n",
    "#X_test=datato3d(X_test)[0]\n",
    "X_test=np.transpose(X_test,(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def loadmat(filename):\n",
    "    '''\n",
    "    读取 MATLAB v7.3 `.mat` 文件（Whole_tracks 作为 tracks）\n",
    "    '''\n",
    "    output = dict()\n",
    "    \n",
    "    # 打开 HDF5 MAT 文件\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        # 读取 Whole_tracks 变量\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'Whole_tracks' 变量不存在！\")\n",
    "\n",
    "        whole_tracks = data['Whole_tracks']  # 结构体 Whole_tracks\n",
    "\n",
    "        # 确保它有 `count` 和 `data`\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"❌ 错误: 'Whole_tracks' 结构不完整！包含: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # 读取 count（可能是字符编码格式，需要解析）\n",
    "        count = whole_tracks['count'][()]  \n",
    "        print(\"🔍 Whole_tracks['count'] 数据:\", count)\n",
    "        print(\"🔍 数据类型:\", type(count))\n",
    "\n",
    "        # 直接转换成整数\n",
    "        total_count = int(count.item())\n",
    "        # 读取 Whole_tracks['data']\n",
    "        track = []\n",
    "        for i in range(total_count):\n",
    "            data_ref = whole_tracks['data'][i].item()\n",
    "            track.append(np.transpose(data[data_ref][:]).astype(np.float32))\n",
    "\n",
    "        # 组织输出\n",
    "        output['tracks'] = {\n",
    "            'count': total_count,\n",
    "            'data': track\n",
    "        }\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 运行测试\n",
    "filename = \"../Testing_Set/J0037_class_label.mat\"\n",
    "tracks_data = loadmat(filename)\n",
    "print(\"✅ 成功读取 Whole_tracks 数据！\")\n",
    "print(f\"轨迹数: {tracks_data['tracks']['count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filename = \"../Testing_Set/J0037_class_label.mat\"\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    print(\"MAT 文件变量:\", list(f.keys()))  # 列出顶层变量\n",
    "    \n",
    "    for key in f.keys():\n",
    "        print(f\"\\n🔍 变量 '{key}' 结构:\")\n",
    "        try:\n",
    "            print(\"    子变量:\", list(f[key].keys()))  # 如果是 group，打印内部结构\n",
    "        except AttributeError:\n",
    "            print(\"    不是结构体，可能是数组或标量\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def load_label_mat(filename):\n",
    "    \"\"\"\n",
    "    读取 MATLAB v7.3 (.mat) 文件中的 `class_label`\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        if 'class_label' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'class_label' 变量不存在！\")\n",
    "\n",
    "        # 读取 class_label\n",
    "        class_label = data['class_label'][()]\n",
    "\n",
    "        # 解析数据\n",
    "        if isinstance(class_label, np.ndarray):\n",
    "            if class_label.size == 1:  # 只有一个值\n",
    "                class_label = class_label.item()\n",
    "            else:  # 多个值，转换为 NumPy 数组\n",
    "                class_label = np.array(class_label)\n",
    "        else:\n",
    "            class_label = int(class_label)  # 可能是单个数值\n",
    "\n",
    "        print(f\"✅ 成功解析 class_label: {class_label}\")\n",
    "        return class_label\n",
    "\n",
    "# 测试\n",
    "filename = \"../Testing_Set/J0037_class_label.mat\"\n",
    "labels = load_label_mat(filename)\n",
    "print(\"📌 解析出的 labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    \"\"\"\n",
    "    通用测试函数（无损失函数计算）\n",
    "    - 适用于分类任务\n",
    "    - 计算预测结果、混淆矩阵、Precision、Recall、F1\n",
    "\n",
    "    参数：\n",
    "    - model: 训练好的 PyTorch 模型\n",
    "    - test_loader: PyTorch DataLoader (测试集)\n",
    "    - device: 'cuda' or 'cpu'\n",
    "\n",
    "    返回：\n",
    "    - conf_matrix: 混淆矩阵\n",
    "    - precision, recall, f1: 分类指标\n",
    "    \"\"\"\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():  # 不计算梯度，加速推理\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # 获取模型输出\n",
    "            output = model(data)\n",
    "\n",
    "            # 分类任务：获取预测类别\n",
    "            if output.dim() > 1:  # 确保 output 是 logits 形式\n",
    "                pred = output.argmax(dim=1)  # 取最大概率的类别\n",
    "                preds.extend(pred.cpu().numpy())\n",
    "                labels.extend(target.cpu().numpy())\n",
    "\n",
    "    # 计算分类指标\n",
    "    if preds:\n",
    "        conf_matrix = confusion_matrix(labels, preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    else:\n",
    "        conf_matrix, precision, recall, f1 = None, None, None, None\n",
    "\n",
    "    # 输出结果\n",
    "    print(f'📊 Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
    "    print(f'🔢 混淆矩阵:\\n{conf_matrix}')\n",
    "\n",
    "    return conf_matrix, precision, recall, f1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deterministic-a-bridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
