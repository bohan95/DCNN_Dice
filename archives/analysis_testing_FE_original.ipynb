{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import h5py\n",
    "import RESNET152_ATT_naive\n",
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.utils.data as utils\n",
    "# 数据加载\n",
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "from Embedding_layer import ROIFeatureExtractor\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import RESNET152_ATT_naive\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "from Util import focalLoss, preprocess_fiber_input\n",
    "from clustering_layer_v2 import ClusterlingLayer\n",
    "from klDiv import KLDivLoss\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmat(filename):\n",
    "    '''\n",
    "    读取 MATLAB v7.3 `.mat` 文件（Whole_tracks 作为 tracks）\n",
    "    '''\n",
    "    output = dict()\n",
    "    \n",
    "    # 打开 HDF5 MAT 文件\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        # 读取 Whole_tracks 变量\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'Whole_tracks' 变量不存在！\")\n",
    "\n",
    "        whole_tracks = data['Whole_tracks']  # 结构体 Whole_tracks\n",
    "\n",
    "        # 确保它有 `count` 和 `data`\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"❌ 错误: 'Whole_tracks' 结构不完整！包含: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # 读取 count（可能是字符编码格式，需要解析）\n",
    "        count = whole_tracks['count'][()]  \n",
    "        print(\"🔍 Whole_tracks['count'] 数据:\", count)\n",
    "        print(\"🔍 数据类型:\", type(count))\n",
    "\n",
    "        # 直接转换成整数\n",
    "        total_count = int(count.item())\n",
    "        print(f'total_count: {total_count}')\n",
    "        # 读取 Whole_tracks['data']\n",
    "        track = []\n",
    "        for i in range(total_count):\n",
    "            data_ref = whole_tracks['data'][i].item()\n",
    "            track.append(np.transpose(data[data_ref][:]).astype(np.float32))\n",
    "\n",
    "        # 组织输出\n",
    "        output['tracks'] = {\n",
    "            'count': total_count,\n",
    "            'data': track\n",
    "        }\n",
    "    \n",
    "    return output\n",
    "\n",
    "def _check_keys(dict):\n",
    "    '''\n",
    "    checks if entries in dictionary are mat-objects. If yes\n",
    "    todict is called to change them to nested dictionaries\n",
    "    '''\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict        \n",
    "\n",
    "def _todict(matobj):\n",
    "    '''\n",
    "    A recursive function which constructs from matobjects nested dictionaries\n",
    "    '''\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict\n",
    "\n",
    "#%%\n",
    "def mySoftmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\"\"\"normalize\"\"\"#110\n",
    "def rescale(X_list,count):\n",
    "    output=list()\n",
    "    if count==1:\n",
    "        output.append(X_list/110)\n",
    "        return output\n",
    "    for i in range(len(X_list)):\n",
    "        output.append(X_list[i]/110)\n",
    "    return output\n",
    "\n",
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "def datato3d(arrays):#list of np arrays, NULL*3*100\n",
    "    output=list()\n",
    "    for i in arrays:\n",
    "        i=np.squeeze(i,axis=1)\n",
    "        i=np.transpose(i,(0,2,1))\n",
    "        output.append(i)\n",
    "    return output\n",
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "    y_nparray = y_nparray.flatten()\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "    \n",
    "def aug_at_test(probs,mode='max'):\n",
    "    assert(len(probs)>0)\n",
    "    if(mode=='max'):\n",
    "        all_probs=np.vstack(probs)\n",
    "        print(all_probs.shape)\n",
    "        max_probs=np.amax(all_probs,axis=1).reshape((2,-1))#row 0: prob for first half, row 1: prob for flipped half\n",
    "        max_idx=np.argmax(max_probs,axis=0)#should be 0/1\n",
    "        test_sample_count=all_probs.shape[0]/2\n",
    "        \n",
    "        class_pred=np.argmax(all_probs,axis=1)\n",
    "        final_pred=list()\n",
    "        for i in range(max_idx.shape[0]):\n",
    "            final_pred.append(class_pred[int(i+test_sample_count*max_idx[i])])#if 0, first half\n",
    "        return final_pred\n",
    "    if(mode=='mean'):\n",
    "        all_probs=np.exp(np.vstack(probs))\n",
    "        test_sample_count=int(all_probs.shape[0]/2)\n",
    "        final_probs=all_probs[0:test_sample_count]+all_probs[test_sample_count:]\n",
    "        final_pred=np.argmax(final_probs,axis=1)\n",
    "        return final_pred.tolist()    \n",
    "\n",
    "def loadmat(filename):\n",
    "    \"\"\" 读取 MATLAB v7.3 .mat 文件 \"\"\"\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'Whole_tracks' 变量不存在！\")\n",
    "        \n",
    "        whole_tracks = data['Whole_tracks']\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"❌ 错误: 'Whole_tracks' 结构不完整！包含: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # 读取 count\n",
    "        count = int(whole_tracks['count'][()].item())\n",
    "        track = [np.transpose(data[whole_tracks['data'][i].item()][:]).astype(np.float32) for i in range(count)]\n",
    "    \n",
    "    return {'tracks': {'count': count, 'data': track}}\n",
    "\n",
    "def load_labels(label_path):\n",
    "    \"\"\" 读取标签 .mat 文件 \"\"\"\n",
    "    with h5py.File(label_path, 'r') as data:\n",
    "        if 'class_label' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'class_label' 变量不存在！\")\n",
    "        \n",
    "        class_label = data['class_label'][()]\n",
    "        \n",
    "        if isinstance(class_label, np.ndarray):\n",
    "            if class_label.size == 1:  \n",
    "                class_label = class_label.item()\n",
    "            else:  \n",
    "                class_label = np.array(class_label)\n",
    "        else:\n",
    "            class_label = int(class_label)\n",
    "\n",
    "        print(f\"✅ 成功解析 class_label, 形状: {class_label.shape}\")\n",
    "        return class_label\n",
    "    \n",
    "def process_file(matpath, label_path, model, roi_extractor, clustering_layer, device, NCLASS, args_test_batch_size):\n",
    "    \"\"\" 处理单个测试文件，并返回其指标 \"\"\"\n",
    "    print(f\"📌 处理数据: {matpath}\")\n",
    "    \n",
    "    mat = loadmat(matpath)\n",
    "    X_test = mat['tracks']['data']\n",
    "    X_test = np.asarray(X_test).astype(np.float32)\n",
    "    X_test_original = np.transpose(X_test, (0, 2, 1))\n",
    "\n",
    "    # 读取标签\n",
    "    y_test = load_labels(label_path)\n",
    "    y_test_list = y_test\n",
    "\n",
    "    # 数据增强\n",
    "    X_test, y_test = udflip(X_test_original, y_test, shuffle=False)\n",
    "\n",
    "    # 转换为 PyTorch Tensor 并移动到相同设备\n",
    "    y_test = torch.from_numpy(y_test.astype(np.int64)).to(device)  # 确保标签也在正确设备上\n",
    "    X_test = torch.from_numpy(X_test).to(device)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    tst_set = utils.TensorDataset(X_test, y_test)\n",
    "    tst_loader = utils.DataLoader(tst_set, batch_size=args_test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # **确保模型和 layers 都在同一个设备**\n",
    "    model.to(device)\n",
    "    roi_extractor.to(device)\n",
    "    clustering_layer.to(device)\n",
    "    model.eval()\n",
    "    roi_extractor.eval()\n",
    "    clustering_layer.eval()\n",
    "\n",
    "    probs, labels = [], []\n",
    "\n",
    "    loss_nll = torch.nn.NLLLoss()\n",
    "    with torch.no_grad():\n",
    "        for data, target in tst_loader:\n",
    "            labels += target.cpu().numpy().tolist()\n",
    "\n",
    "            # 确保 data 和 target 都在同一设备\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # 预处理数据\n",
    "            data_processed = preprocess_fiber_input(data, roi_extractor=roi_extractor, device=device, net_type='FE')\n",
    "\n",
    "            # 送入模型\n",
    "            output, embed, *_ = model(data_processed)  # **确保 model 已被移动到 `device`**\n",
    "\n",
    "            probs.append(output.data.cpu().numpy())  # 确保 probs 存储在 CPU\n",
    "\n",
    "    # 计算最终预测\n",
    "    preds = aug_at_test(probs, mode='max')\n",
    "\n",
    "    # 计算指标\n",
    "    conf_mat = confusion_matrix(y_test_list, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test_list, preds, average='macro')\n",
    "\n",
    "    try:\n",
    "        probs = np.concatenate(probs, axis=0)\n",
    "        probs = F.softmax(torch.tensor(probs), dim=1).numpy()\n",
    "        labels = np.array(labels)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "\n",
    "        auroc = roc_auc_score(labels, probs, multi_class='ovr')\n",
    "        auprc = average_precision_score(labels, probs, average='macro')\n",
    "    except ValueError as e:\n",
    "        print(f\"AUROC / AUPRC 计算错误: {e}\")\n",
    "        auroc, auprc = None, None\n",
    "\n",
    "    return precision, recall, f1, auroc, auprc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 处理数据: ../Testing_Set/J0037_tracks.mat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.utils.data as utils\n",
    "\n",
    "# 参数\n",
    "matpath = '../Testing_Set/J0037_tracks.mat'\n",
    "label_path = '../Testing_Set/J0037_class_label.mat'  # 你的标签文件\n",
    "classnum = 15  # 类别数\n",
    "ROI_EMBEDDING_DIM = 32\n",
    "\n",
    "def loadmat(filename):\n",
    "    \"\"\" 读取 MATLAB v7.3 .mat 文件 \"\"\"\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'Whole_tracks' 变量不存在！\")\n",
    "        \n",
    "        whole_tracks = data['Whole_tracks']\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"❌ 错误: 'Whole_tracks' 结构不完整！包含: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # 读取 count\n",
    "        count = int(whole_tracks['count'][()].item())\n",
    "        track = [np.transpose(data[whole_tracks['data'][i].item()][:]).astype(np.float32) for i in range(count)]\n",
    "    \n",
    "    return {'tracks': {'count': count, 'data': track}}\n",
    "\n",
    "def load_labels(label_path):\n",
    "    \"\"\" 读取标签 .mat 文件 \"\"\"\n",
    "    with h5py.File(label_path, 'r') as data:\n",
    "        if 'class_label' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'class_label' 变量不存在！\")\n",
    "        \n",
    "        class_label = data['class_label'][()]\n",
    "        \n",
    "        if isinstance(class_label, np.ndarray):\n",
    "            if class_label.size == 1:  \n",
    "                class_label = class_label.item()\n",
    "            else:  \n",
    "                class_label = np.array(class_label)\n",
    "        else:\n",
    "            class_label = int(class_label)\n",
    "\n",
    "        print(f\"✅ 成功解析 class_label, 形状: {class_label.shape}\")\n",
    "        return class_label\n",
    "\n",
    "\n",
    "\"\"\" 测试模型 \"\"\"\n",
    "args_test_batch_size = 10000\n",
    "NCLASS = int(classnum)\n",
    "\n",
    "print(f\"📌 处理数据: {matpath}\")\n",
    "mat = loadmat(matpath)\n",
    "X_test = mat['tracks']['data']\n",
    "X_test = np.asarray(X_test).astype(np.float32)\n",
    "X_test_original = np.transpose(X_test, (0, 2, 1))  # 维度转换\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功解析 class_label, 形状: (234541, 1)\n",
      "(234541, 4, 100)\n",
      "(234541, 1)\n",
      "(469082, 4, 100)\n",
      "(469082,)\n"
     ]
    }
   ],
   "source": [
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "    y_nparray = y_nparray.flatten()\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "# 读取标签\n",
    "y_test = load_labels(label_path)\n",
    "y_test_list = y_test\n",
    "print(X_test_original.shape)\n",
    "print(y_test.shape)\n",
    "X_test, y_test = udflip(X_test_original,y_test,shuffle=False)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_test_np = X_test.copy()\n",
    "y_test = torch.from_numpy(y_test.astype(np.int64))  # 确保是整数类型\n",
    "X_test = torch.from_numpy(X_test)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "tst_set = utils.TensorDataset(X_test, y_test)\n",
    "tst_loader = utils.DataLoader(tst_set, batch_size=args_test_batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469082"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([469082, 4, 100]), torch.Size([469082]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_at_test(probs,mode='max'):\n",
    "    assert(len(probs)>0)\n",
    "    if(mode=='max'):\n",
    "        all_probs=np.vstack(probs)\n",
    "        print(all_probs.shape)\n",
    "        max_probs=np.amax(all_probs,axis=1).reshape((2,-1))#row 0: prob for first half, row 1: prob for flipped half\n",
    "        max_idx=np.argmax(max_probs,axis=0)#should be 0/1\n",
    "        test_sample_count=all_probs.shape[0]/2\n",
    "        \n",
    "        class_pred=np.argmax(all_probs,axis=1)\n",
    "        final_pred=list()\n",
    "        for i in range(max_idx.shape[0]):\n",
    "            final_pred.append(class_pred[int(i+test_sample_count*max_idx[i])])#if 0, first half\n",
    "        return final_pred\n",
    "    if(mode=='mean'):\n",
    "        all_probs=np.exp(np.vstack(probs))\n",
    "        test_sample_count=int(all_probs.shape[0]/2)\n",
    "        final_probs=all_probs[0:test_sample_count]+all_probs[test_sample_count:]\n",
    "        final_pred=np.argmax(final_probs,axis=1)\n",
    "        return final_pred.tolist()\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def aug_at_test(probs, mode='max'):\n",
    "#     \"\"\"\n",
    "#     适用于 **无数据增强** 的版本：\n",
    "#     - 直接选择 `argmax` 作为最终预测\n",
    "#     - `mode='max'` 或 `mode='mean'` 影响不大，因为没有翻转数据\n",
    "    \n",
    "#     参数：\n",
    "#     - probs: 模型输出的 logits 列表，每个 batch 存储一次输出\n",
    "    \n",
    "#     返回：\n",
    "#     - final_pred: 预测类别列表\n",
    "#     \"\"\"\n",
    "#     assert len(probs) > 0, \"probs 为空，无法计算预测结果\"\n",
    "\n",
    "#     # 合并所有 batch\n",
    "#     all_probs = np.vstack(probs)  # 形状: (N, num_classes)\n",
    "\n",
    "#     # 直接取最大概率类别作为预测类别\n",
    "#     final_pred = np.argmax(all_probs, axis=1)\n",
    "\n",
    "#     return final_pred.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, roi_extractor, tst_loader, device):\n",
    "    model.eval()\n",
    "    logit=list()\n",
    "    attVec=list()\n",
    "    for data,lbl in tst_loader:\n",
    "        with torch.no_grad():  # error corrected by MH 10/12/2022 (add with torch.no_grad():) \n",
    "            data = Variable(data.cuda())\n",
    "            data_processed = preprocess_fiber_input(data, roi_extractor=roi_extractor, device=device, net_type='FE')\n",
    "            output,_,att,_, _, _, _, _, _, _, _ = model(data_processed)\n",
    "            logit.append(output.data.cpu().numpy())\n",
    "            attVec.append(att.data.cpu().numpy())\n",
    "    return logit,attVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:1\n",
      "using ROI with emb: 32\n",
      "size of attVec (469082, 100)\n",
      "results saved!\n"
     ]
    }
   ],
   "source": [
    "# 数据加载\n",
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "from Embedding_layer import ROIFeatureExtractor\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import RESNET152_ATT_naive\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "from Util import focalLoss, preprocess_fiber_input\n",
    "from clustering_layer_v2 import ClusterlingLayer\n",
    "from klDiv import KLDivLoss\n",
    "\n",
    "modelpath = 'focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "fe_path = 'FE_layer_focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "cls_path = 'CLS_layer_focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "# 加载模型\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "ROI_EMBEDDING_DIM = 32\n",
    "NUM_ROI_CLASSES = 726 + 1\n",
    "HIDDEN_DIM = 64\n",
    "model=RESNET152_ATT_naive.resnet18(num_classes=NCLASS, input_ch=3+ROI_EMBEDDING_DIM)\n",
    "# init ROI Embedding layer\n",
    "roi_embedding_layer = nn.Embedding(NUM_ROI_CLASSES, ROI_EMBEDDING_DIM).to(device)\n",
    "# init FE\n",
    "roi_extractor = ROIFeatureExtractor(roi_embedding_layer, ROI_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM).to(device)\n",
    "roi_extractor.to(device)\n",
    "model.to(device)\n",
    "clustering_layer = ClusterlingLayer(embedding_dimension=512, num_clusters=NCLASS, alpha=1.0)\n",
    "kl_loss = KLDivLoss(NCLASS, loss_weight=2.0, temperature=2)\n",
    "kl_loss.to(device)\n",
    "clustering_layer.to(device)\n",
    "# 2️⃣ 加载权重\n",
    "state_dict = torch.load(modelpath, map_location=device)\n",
    "state_dict_FE = torch.load(fe_path, map_location=device)\n",
    "state_dict_cls = torch.load(cls_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "roi_extractor.load_state_dict(state_dict_FE)\n",
    "clustering_layer.load_state_dict(state_dict_cls)\n",
    "# 0.9115309895465665 0.919601178493508 0.913951033276079  \n",
    "# 0.9038265169218341 0.9223096121975994 0.9100564454400993\n",
    "model.eval()\n",
    "roi_extractor.eval()\n",
    "clustering_layer.eval()\n",
    "\n",
    "log_testing_total_loss = 0.0\n",
    "log_focal_loss = 0.0\n",
    "log_centering_loss= 0.\n",
    "log_clustering_loss = 0.0\n",
    "probs = []\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "global global_cluster_rois  # Ensure global access to cluster anatomical profiles\n",
    "loss_nll = nn.NLLLoss(size_average=True) # log-softmax applied in the network\n",
    "with torch.no_grad():\n",
    "    logit, attVec = test(model, roi_extractor, tst_loader, device)\n",
    "    attVec=mySoftmax(np.squeeze(np.vstack(attVec))).astype(np.float32)\n",
    "    print('size of attVec',attVec.shape)\n",
    "    #build output\n",
    "    prob=np.exp(np.vstack(logit)).astype(np.float32)\n",
    "    membership=np.argmax(prob,axis=1).reshape((-1,1)).astype(np.float32)\n",
    "    maxprob=np.amax(prob,axis=1).reshape((-1,1)).astype(np.float32)\n",
    "    \n",
    "    output_max=np.zeros((X_test_np.shape[0],7),dtype=np.float32)\n",
    "    output_max[:,0]=np.arange(1,X_test_np.shape[0]+1)\n",
    "    for i in range(X_test_np.shape[0]):\n",
    "        output_max[i, 0] = i + 1  # Fiber ID（从 1 开始）\n",
    "        output_max[i, 1:4] = X_test_np[i, 0:3, 0]  # 取前三个通道的坐标\n",
    "        output_max[i, 4] = X_test_np[i, 3, 0]  # 存储 ROI 信息\n",
    "    #merge\n",
    "    output_max=np.hstack((output_max,prob,membership,maxprob))\n",
    "    np.savetxt(matpath.replace('.mat','.txt'),output_max,fmt='%.4e')\n",
    "        # Compute clustering loss if enabled\n",
    "    for i in range(NCLASS):\n",
    "        #print(i)\n",
    "        submat=output_max[np.where(output_max[:,-2]==i)]\n",
    "        #fiber index\n",
    "        fiberIndex=submat[:,0].reshape((-1,1))\n",
    "        np.savetxt(matpath.replace('.mat','_'+'{0:02}'.format(i)+'_fiberindex.txt'),fiberIndex,fmt='%d')\n",
    "        #fiber prob\n",
    "        fiberProb=submat[:,-1].reshape((-1,1))\n",
    "        np.savetxt(matpath.replace('.mat','_'+'{0:02}'.format(i)+'_fiberprob.txt'),fiberProb,fmt='%.4e')\n",
    "        #fiber attention map\n",
    "        fiberAtm=attVec[np.where(output_max[:,-2]==i)]\n",
    "        #print('fiberAtm.shape',fiberAtm.shape)\n",
    "        np.savetxt(matpath.replace('.mat','_'+'{0:02}'.format(i)+'_fiberatm.txt'),fiberAtm,fmt='%.4e')        \n",
    "    print('results saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_np shape: (469082, 4, 100)\n",
      "Sample data: [[-65.00016   -64.53611   -63.89756   -63.2625    -62.64133   -62.020386\n",
      "  -61.442703  -60.896885  -60.38636   -59.964584  -59.535625  -58.96403\n",
      "  -58.362305  -57.720272  -57.068596  -56.503193  -55.987263  -55.544903\n",
      "  -55.112076  -54.632645  -54.070496  -53.521496  -52.960003  -52.42095\n",
      "  -51.86682   -51.236607  -50.614723  -49.98778   -49.344894  -48.761734\n",
      "  -48.19622   -47.72383   -47.265675  -46.75627   -46.230648  -45.641064\n",
      "  -45.04124   -44.43267   -43.778633  -43.20743   -42.64826   -42.0742\n",
      "  -41.46937   -40.846848  -40.252407  -39.64069   -39.064342  -38.444927\n",
      "  -37.86092   -37.244854  -36.685444  -36.114853  -35.611057  -35.092754\n",
      "  -34.570095  -33.9988    -33.416508  -32.779728  -32.186512  -31.600033\n",
      "  -31.10708   -30.683212  -30.342094  -30.116259  -30.05608   -30.042112\n",
      "  -30.039797  -30.0403    -30.003862  -29.942793  -29.870632  -29.778378\n",
      "  -29.690678  -29.59237   -29.5686    -29.628315  -29.716938  -29.862934\n",
      "  -29.941267  -29.901352  -29.753572  -29.410868  -29.123024  -28.901161\n",
      "  -28.693308  -28.46276   -28.157333  -27.758356  -27.398674  -27.0124\n",
      "  -26.58169   -26.11781   -25.734016  -25.354046  -24.957836  -24.504557\n",
      "  -23.927526  -23.303854  -22.68994   -22.073053 ]\n",
      " [ -8.238065   -8.580183   -8.8432045  -8.958588   -9.08037    -9.291477\n",
      "   -9.45204    -9.549636   -9.538213   -9.561714   -9.618327   -9.722491\n",
      "   -9.763809   -9.761576   -9.747072   -9.78423    -9.891859  -10.108349\n",
      "  -10.401012  -10.742704  -10.911076  -11.009087  -11.078047  -11.195415\n",
      "  -11.328724  -11.479393  -11.610501  -11.753139  -11.940948  -12.140582\n",
      "  -12.35845   -12.556676  -12.749454  -12.9603405 -13.049337  -13.056522\n",
      "  -13.020525  -13.022592  -13.15476   -13.404909  -13.778038  -14.043396\n",
      "  -14.260012  -14.413137  -14.578808  -14.810324  -15.037966  -15.2884\n",
      "  -15.513169  -15.744109  -15.953544  -16.176174  -16.409868  -16.666279\n",
      "  -16.91908   -17.185207  -17.404074  -17.605753  -17.779652  -17.913683\n",
      "  -17.903522  -17.817133  -17.683447  -17.53351   -17.381615  -17.230461\n",
      "  -17.073675  -16.935324  -16.898388  -16.911665  -16.889     -16.844305\n",
      "  -16.81506   -16.79527   -16.801685  -16.841064  -16.87111   -16.887484\n",
      "  -16.900515  -16.92148   -16.9102    -16.833618  -16.754072  -16.669834\n",
      "  -16.596594  -16.525682  -16.492758  -16.493254  -16.545738  -16.653032\n",
      "  -16.836157  -17.083649  -17.386856  -17.709454  -17.971918  -18.173738\n",
      "  -18.248573  -18.241375  -18.114214  -17.94241  ]\n",
      " [ 16.675165   16.793867   16.945707   17.008297   17.078043   17.206673\n",
      "   17.42932    17.742977   18.175137   18.65538    19.128855   19.457705\n",
      "   19.638859   19.750326   19.892792   20.161436   20.546204   20.99239\n",
      "   21.348288   21.640076   21.924244   22.225294   22.573952   22.919245\n",
      "   23.191216   23.355795   23.40482    23.415377   23.440792   23.567942\n",
      "   23.815664   24.219835   24.620846   25.00289    25.332167   25.642715\n",
      "   25.890715   26.064068   26.118364   26.14348    26.160908   26.220228\n",
      "   26.314604   26.458878   26.603775   26.751947   26.847668   26.904442\n",
      "   26.9661     27.059477   27.257385   27.527239   27.834015   28.169498\n",
      "   28.430708   28.656418   28.777506   28.807789   28.673594   28.396498\n",
      "   27.99665    27.507551   26.969078   26.389297   25.757341   25.113157\n",
      "   24.484318   23.836231   23.208824   22.541155   21.921213   21.25586\n",
      "   20.639437   19.971409   19.35232    18.683422   18.062021   17.400259\n",
      "   16.779535   16.104458   15.502725   14.93447    14.383096   13.754068\n",
      "   13.165487   12.536086   11.99419    11.454729   10.937906   10.406352\n",
      "    9.98093     9.574791    9.1641655   8.738934    8.293832    7.87914\n",
      "    7.6011524   7.38561     7.236625    7.115001 ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.        145.          0.          0.          0.\n",
      "    0.          0.          0.        160.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        726.\n",
      "  726.        726.        726.        726.        726.        726.\n",
      "  726.        726.        726.        726.        726.        726.\n",
      "  726.        726.        726.        726.        726.        726.\n",
      "  726.        726.        726.        726.       ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test_np shape:\", X_test_np.shape)\n",
    "print(\"Sample data:\", X_test_np[0])  # 打印第一个样本\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deterministic-a-bridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
