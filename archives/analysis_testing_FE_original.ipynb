{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import h5py\n",
    "import RESNET152_ATT_naive\n",
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.utils.data as utils\n",
    "# Êï∞ÊçÆÂä†ËΩΩ\n",
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "from Embedding_layer import ROIFeatureExtractor\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import RESNET152_ATT_naive\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "from Util import focalLoss, preprocess_fiber_input\n",
    "from clustering_layer_v2 import ClusterlingLayer\n",
    "from klDiv import KLDivLoss\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmat(filename):\n",
    "    '''\n",
    "    ËØªÂèñ MATLAB v7.3 `.mat` Êñá‰ª∂ÔºàWhole_tracks ‰Ωú‰∏∫ tracksÔºâ\n",
    "    '''\n",
    "    output = dict()\n",
    "    \n",
    "    # ÊâìÂºÄ HDF5 MAT Êñá‰ª∂\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        # ËØªÂèñ Whole_tracks ÂèòÈáè\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"‚ùå ÈîôËØØ: 'Whole_tracks' ÂèòÈáè‰∏çÂ≠òÂú®ÔºÅ\")\n",
    "\n",
    "        whole_tracks = data['Whole_tracks']  # ÁªìÊûÑ‰Ωì Whole_tracks\n",
    "\n",
    "        # Á°Æ‰øùÂÆÉÊúâ `count` Âíå `data`\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"‚ùå ÈîôËØØ: 'Whole_tracks' ÁªìÊûÑ‰∏çÂÆåÊï¥ÔºÅÂåÖÂê´: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # ËØªÂèñ countÔºàÂèØËÉΩÊòØÂ≠óÁ¨¶ÁºñÁ†ÅÊ†ºÂºèÔºåÈúÄË¶ÅËß£ÊûêÔºâ\n",
    "        count = whole_tracks['count'][()]  \n",
    "        print(\"üîç Whole_tracks['count'] Êï∞ÊçÆ:\", count)\n",
    "        print(\"üîç Êï∞ÊçÆÁ±ªÂûã:\", type(count))\n",
    "\n",
    "        # Áõ¥Êé•ËΩ¨Êç¢ÊàêÊï¥Êï∞\n",
    "        total_count = int(count.item())\n",
    "        print(f'total_count: {total_count}')\n",
    "        # ËØªÂèñ Whole_tracks['data']\n",
    "        track = []\n",
    "        for i in range(total_count):\n",
    "            data_ref = whole_tracks['data'][i].item()\n",
    "            track.append(np.transpose(data[data_ref][:]).astype(np.float32))\n",
    "\n",
    "        # ÁªÑÁªáËæìÂá∫\n",
    "        output['tracks'] = {\n",
    "            'count': total_count,\n",
    "            'data': track\n",
    "        }\n",
    "    \n",
    "    return output\n",
    "\n",
    "def _check_keys(dict):\n",
    "    '''\n",
    "    checks if entries in dictionary are mat-objects. If yes\n",
    "    todict is called to change them to nested dictionaries\n",
    "    '''\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict        \n",
    "\n",
    "def _todict(matobj):\n",
    "    '''\n",
    "    A recursive function which constructs from matobjects nested dictionaries\n",
    "    '''\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict\n",
    "\n",
    "#%%\n",
    "def mySoftmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\"\"\"normalize\"\"\"#110\n",
    "def rescale(X_list,count):\n",
    "    output=list()\n",
    "    if count==1:\n",
    "        output.append(X_list/110)\n",
    "        return output\n",
    "    for i in range(len(X_list)):\n",
    "        output.append(X_list[i]/110)\n",
    "    return output\n",
    "\n",
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "def datato3d(arrays):#list of np arrays, NULL*3*100\n",
    "    output=list()\n",
    "    for i in arrays:\n",
    "        i=np.squeeze(i,axis=1)\n",
    "        i=np.transpose(i,(0,2,1))\n",
    "        output.append(i)\n",
    "    return output\n",
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "    y_nparray = y_nparray.flatten()\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "    \n",
    "def aug_at_test(probs,mode='max'):\n",
    "    assert(len(probs)>0)\n",
    "    if(mode=='max'):\n",
    "        all_probs=np.vstack(probs)\n",
    "        print(all_probs.shape)\n",
    "        max_probs=np.amax(all_probs,axis=1).reshape((2,-1))#row 0: prob for first half, row 1: prob for flipped half\n",
    "        max_idx=np.argmax(max_probs,axis=0)#should be 0/1\n",
    "        test_sample_count=all_probs.shape[0]/2\n",
    "        \n",
    "        class_pred=np.argmax(all_probs,axis=1)\n",
    "        final_pred=list()\n",
    "        for i in range(max_idx.shape[0]):\n",
    "            final_pred.append(class_pred[int(i+test_sample_count*max_idx[i])])#if 0, first half\n",
    "        return final_pred\n",
    "    if(mode=='mean'):\n",
    "        all_probs=np.exp(np.vstack(probs))\n",
    "        test_sample_count=int(all_probs.shape[0]/2)\n",
    "        final_probs=all_probs[0:test_sample_count]+all_probs[test_sample_count:]\n",
    "        final_pred=np.argmax(final_probs,axis=1)\n",
    "        return final_pred.tolist()    \n",
    "\n",
    "def loadmat(filename):\n",
    "    \"\"\" ËØªÂèñ MATLAB v7.3 .mat Êñá‰ª∂ \"\"\"\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"‚ùå ÈîôËØØ: 'Whole_tracks' ÂèòÈáè‰∏çÂ≠òÂú®ÔºÅ\")\n",
    "        \n",
    "        whole_tracks = data['Whole_tracks']\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"‚ùå ÈîôËØØ: 'Whole_tracks' ÁªìÊûÑ‰∏çÂÆåÊï¥ÔºÅÂåÖÂê´: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # ËØªÂèñ count\n",
    "        count = int(whole_tracks['count'][()].item())\n",
    "        track = [np.transpose(data[whole_tracks['data'][i].item()][:]).astype(np.float32) for i in range(count)]\n",
    "    \n",
    "    return {'tracks': {'count': count, 'data': track}}\n",
    "\n",
    "def load_labels(label_path):\n",
    "    \"\"\" ËØªÂèñÊ†áÁ≠æ .mat Êñá‰ª∂ \"\"\"\n",
    "    with h5py.File(label_path, 'r') as data:\n",
    "        if 'class_label' not in data:\n",
    "            raise KeyError(\"‚ùå ÈîôËØØ: 'class_label' ÂèòÈáè‰∏çÂ≠òÂú®ÔºÅ\")\n",
    "        \n",
    "        class_label = data['class_label'][()]\n",
    "        \n",
    "        if isinstance(class_label, np.ndarray):\n",
    "            if class_label.size == 1:  \n",
    "                class_label = class_label.item()\n",
    "            else:  \n",
    "                class_label = np.array(class_label)\n",
    "        else:\n",
    "            class_label = int(class_label)\n",
    "\n",
    "        print(f\"‚úÖ ÊàêÂäüËß£Êûê class_label, ÂΩ¢Áä∂: {class_label.shape}\")\n",
    "        return class_label\n",
    "    \n",
    "def process_file(matpath, label_path, model, roi_extractor, clustering_layer, device, NCLASS, args_test_batch_size):\n",
    "    \"\"\" Â§ÑÁêÜÂçï‰∏™ÊµãËØïÊñá‰ª∂ÔºåÂπ∂ËøîÂõûÂÖ∂ÊåáÊ†á \"\"\"\n",
    "    print(f\"üìå Â§ÑÁêÜÊï∞ÊçÆ: {matpath}\")\n",
    "    \n",
    "    mat = loadmat(matpath)\n",
    "    X_test = mat['tracks']['data']\n",
    "    X_test = np.asarray(X_test).astype(np.float32)\n",
    "    X_test_original = np.transpose(X_test, (0, 2, 1))\n",
    "\n",
    "    # ËØªÂèñÊ†áÁ≠æ\n",
    "    y_test = load_labels(label_path)\n",
    "    y_test_list = y_test\n",
    "\n",
    "    # Êï∞ÊçÆÂ¢ûÂº∫\n",
    "    X_test, y_test = udflip(X_test_original, y_test, shuffle=False)\n",
    "\n",
    "    # ËΩ¨Êç¢‰∏∫ PyTorch Tensor Âπ∂ÁßªÂä®Âà∞Áõ∏ÂêåËÆæÂ§á\n",
    "    y_test = torch.from_numpy(y_test.astype(np.int64)).to(device)  # Á°Æ‰øùÊ†áÁ≠æ‰πüÂú®Ê≠£Á°ÆËÆæÂ§á‰∏ä\n",
    "    X_test = torch.from_numpy(X_test).to(device)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    tst_set = utils.TensorDataset(X_test, y_test)\n",
    "    tst_loader = utils.DataLoader(tst_set, batch_size=args_test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # **Á°Æ‰øùÊ®°ÂûãÂíå layers ÈÉΩÂú®Âêå‰∏Ä‰∏™ËÆæÂ§á**\n",
    "    model.to(device)\n",
    "    roi_extractor.to(device)\n",
    "    clustering_layer.to(device)\n",
    "    model.eval()\n",
    "    roi_extractor.eval()\n",
    "    clustering_layer.eval()\n",
    "\n",
    "    probs, labels = [], []\n",
    "\n",
    "    loss_nll = torch.nn.NLLLoss()\n",
    "    with torch.no_grad():\n",
    "        for data, target in tst_loader:\n",
    "            labels += target.cpu().numpy().tolist()\n",
    "\n",
    "            # Á°Æ‰øù data Âíå target ÈÉΩÂú®Âêå‰∏ÄËÆæÂ§á\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # È¢ÑÂ§ÑÁêÜÊï∞ÊçÆ\n",
    "            data_processed = preprocess_fiber_input(data, roi_extractor=roi_extractor, device=device, net_type='FE')\n",
    "\n",
    "            # ÈÄÅÂÖ•Ê®°Âûã\n",
    "            output, embed, *_ = model(data_processed)  # **Á°Æ‰øù model Â∑≤Ë¢´ÁßªÂä®Âà∞ `device`**\n",
    "\n",
    "            probs.append(output.data.cpu().numpy())  # Á°Æ‰øù probs Â≠òÂÇ®Âú® CPU\n",
    "\n",
    "    # ËÆ°ÁÆóÊúÄÁªàÈ¢ÑÊµã\n",
    "    preds = aug_at_test(probs, mode='max')\n",
    "\n",
    "    # ËÆ°ÁÆóÊåáÊ†á\n",
    "    conf_mat = confusion_matrix(y_test_list, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test_list, preds, average='macro')\n",
    "\n",
    "    try:\n",
    "        probs = np.concatenate(probs, axis=0)\n",
    "        probs = F.softmax(torch.tensor(probs), dim=1).numpy()\n",
    "        labels = np.array(labels)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "\n",
    "        auroc = roc_auc_score(labels, probs, multi_class='ovr')\n",
    "        auprc = average_precision_score(labels, probs, average='macro')\n",
    "    except ValueError as e:\n",
    "        print(f\"AUROC / AUPRC ËÆ°ÁÆóÈîôËØØ: {e}\")\n",
    "        auroc, auprc = None, None\n",
    "\n",
    "    return precision, recall, f1, auroc, auprc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Â§ÑÁêÜÊï∞ÊçÆ: ../Testing_Set/J0037_tracks.mat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.utils.data as utils\n",
    "\n",
    "# ÂèÇÊï∞\n",
    "matpath = '../Testing_Set/J0037_tracks.mat'\n",
    "label_path = '../Testing_Set/J0037_class_label.mat'  # ‰Ω†ÁöÑÊ†áÁ≠æÊñá‰ª∂\n",
    "classnum = 15  # Á±ªÂà´Êï∞\n",
    "ROI_EMBEDDING_DIM = 32\n",
    "\n",
    "def loadmat(filename):\n",
    "    \"\"\" ËØªÂèñ MATLAB v7.3 .mat Êñá‰ª∂ \"\"\"\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"‚ùå ÈîôËØØ: 'Whole_tracks' ÂèòÈáè‰∏çÂ≠òÂú®ÔºÅ\")\n",
    "        \n",
    "        whole_tracks = data['Whole_tracks']\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"‚ùå ÈîôËØØ: 'Whole_tracks' ÁªìÊûÑ‰∏çÂÆåÊï¥ÔºÅÂåÖÂê´: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # ËØªÂèñ count\n",
    "        count = int(whole_tracks['count'][()].item())\n",
    "        track = [np.transpose(data[whole_tracks['data'][i].item()][:]).astype(np.float32) for i in range(count)]\n",
    "    \n",
    "    return {'tracks': {'count': count, 'data': track}}\n",
    "\n",
    "def load_labels(label_path):\n",
    "    \"\"\" ËØªÂèñÊ†áÁ≠æ .mat Êñá‰ª∂ \"\"\"\n",
    "    with h5py.File(label_path, 'r') as data:\n",
    "        if 'class_label' not in data:\n",
    "            raise KeyError(\"‚ùå ÈîôËØØ: 'class_label' ÂèòÈáè‰∏çÂ≠òÂú®ÔºÅ\")\n",
    "        \n",
    "        class_label = data['class_label'][()]\n",
    "        \n",
    "        if isinstance(class_label, np.ndarray):\n",
    "            if class_label.size == 1:  \n",
    "                class_label = class_label.item()\n",
    "            else:  \n",
    "                class_label = np.array(class_label)\n",
    "        else:\n",
    "            class_label = int(class_label)\n",
    "\n",
    "        print(f\"‚úÖ ÊàêÂäüËß£Êûê class_label, ÂΩ¢Áä∂: {class_label.shape}\")\n",
    "        return class_label\n",
    "\n",
    "\n",
    "\"\"\" ÊµãËØïÊ®°Âûã \"\"\"\n",
    "args_test_batch_size = 10000\n",
    "NCLASS = int(classnum)\n",
    "\n",
    "print(f\"üìå Â§ÑÁêÜÊï∞ÊçÆ: {matpath}\")\n",
    "mat = loadmat(matpath)\n",
    "X_test = mat['tracks']['data']\n",
    "X_test = np.asarray(X_test).astype(np.float32)\n",
    "X_test_original = np.transpose(X_test, (0, 2, 1))  # Áª¥Â∫¶ËΩ¨Êç¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÊàêÂäüËß£Êûê class_label, ÂΩ¢Áä∂: (234541, 1)\n",
      "(234541, 4, 100)\n",
      "(234541, 1)\n",
      "(469082, 4, 100)\n",
      "(469082,)\n"
     ]
    }
   ],
   "source": [
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "    y_nparray = y_nparray.flatten()\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "# ËØªÂèñÊ†áÁ≠æ\n",
    "y_test = load_labels(label_path)\n",
    "y_test_list = y_test\n",
    "print(X_test_original.shape)\n",
    "print(y_test.shape)\n",
    "X_test, y_test = udflip(X_test_original,y_test,shuffle=False)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_test_np = X_test.copy()\n",
    "y_test = torch.from_numpy(y_test.astype(np.int64))  # Á°Æ‰øùÊòØÊï¥Êï∞Á±ªÂûã\n",
    "X_test = torch.from_numpy(X_test)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "tst_set = utils.TensorDataset(X_test, y_test)\n",
    "tst_loader = utils.DataLoader(tst_set, batch_size=args_test_batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469082"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([469082, 4, 100]), torch.Size([469082]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_at_test(probs,mode='max'):\n",
    "    assert(len(probs)>0)\n",
    "    if(mode=='max'):\n",
    "        all_probs=np.vstack(probs)\n",
    "        print(all_probs.shape)\n",
    "        max_probs=np.amax(all_probs,axis=1).reshape((2,-1))#row 0: prob for first half, row 1: prob for flipped half\n",
    "        max_idx=np.argmax(max_probs,axis=0)#should be 0/1\n",
    "        test_sample_count=all_probs.shape[0]/2\n",
    "        \n",
    "        class_pred=np.argmax(all_probs,axis=1)\n",
    "        final_pred=list()\n",
    "        for i in range(max_idx.shape[0]):\n",
    "            final_pred.append(class_pred[int(i+test_sample_count*max_idx[i])])#if 0, first half\n",
    "        return final_pred\n",
    "    if(mode=='mean'):\n",
    "        all_probs=np.exp(np.vstack(probs))\n",
    "        test_sample_count=int(all_probs.shape[0]/2)\n",
    "        final_probs=all_probs[0:test_sample_count]+all_probs[test_sample_count:]\n",
    "        final_pred=np.argmax(final_probs,axis=1)\n",
    "        return final_pred.tolist()\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def aug_at_test(probs, mode='max'):\n",
    "#     \"\"\"\n",
    "#     ÈÄÇÁî®‰∫é **Êó†Êï∞ÊçÆÂ¢ûÂº∫** ÁöÑÁâàÊú¨Ôºö\n",
    "#     - Áõ¥Êé•ÈÄâÊã© `argmax` ‰Ωú‰∏∫ÊúÄÁªàÈ¢ÑÊµã\n",
    "#     - `mode='max'` Êàñ `mode='mean'` ÂΩ±Âìç‰∏çÂ§ßÔºåÂõ†‰∏∫Ê≤°ÊúâÁøªËΩ¨Êï∞ÊçÆ\n",
    "    \n",
    "#     ÂèÇÊï∞Ôºö\n",
    "#     - probs: Ê®°ÂûãËæìÂá∫ÁöÑ logits ÂàóË°®ÔºåÊØè‰∏™ batch Â≠òÂÇ®‰∏ÄÊ¨°ËæìÂá∫\n",
    "    \n",
    "#     ËøîÂõûÔºö\n",
    "#     - final_pred: È¢ÑÊµãÁ±ªÂà´ÂàóË°®\n",
    "#     \"\"\"\n",
    "#     assert len(probs) > 0, \"probs ‰∏∫Á©∫ÔºåÊó†Ê≥ïËÆ°ÁÆóÈ¢ÑÊµãÁªìÊûú\"\n",
    "\n",
    "#     # ÂêàÂπ∂ÊâÄÊúâ batch\n",
    "#     all_probs = np.vstack(probs)  # ÂΩ¢Áä∂: (N, num_classes)\n",
    "\n",
    "#     # Áõ¥Êé•ÂèñÊúÄÂ§ßÊ¶ÇÁéáÁ±ªÂà´‰Ωú‰∏∫È¢ÑÊµãÁ±ªÂà´\n",
    "#     final_pred = np.argmax(all_probs, axis=1)\n",
    "\n",
    "#     return final_pred.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, roi_extractor, tst_loader, device):\n",
    "    model.eval()\n",
    "    logit=list()\n",
    "    attVec=list()\n",
    "    for data,lbl in tst_loader:\n",
    "        with torch.no_grad():  # error corrected by MH 10/12/2022 (add with torch.no_grad():) \n",
    "            data = Variable(data.cuda())\n",
    "            data_processed = preprocess_fiber_input(data, roi_extractor=roi_extractor, device=device, net_type='FE')\n",
    "            output,_,att,_, _, _, _, _, _, _, _ = model(data_processed)\n",
    "            logit.append(output.data.cpu().numpy())\n",
    "            attVec.append(att.data.cpu().numpy())\n",
    "    return logit,attVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:1\n",
      "using ROI with emb: 32\n",
      "size of attVec (469082, 100)\n",
      "results saved!\n"
     ]
    }
   ],
   "source": [
    "# Êï∞ÊçÆÂä†ËΩΩ\n",
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "from Embedding_layer import ROIFeatureExtractor\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import RESNET152_ATT_naive\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "from Util import focalLoss, preprocess_fiber_input\n",
    "from clustering_layer_v2 import ClusterlingLayer\n",
    "from klDiv import KLDivLoss\n",
    "\n",
    "modelpath = 'focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "fe_path = 'FE_layer_focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "cls_path = 'CLS_layer_focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "# Âä†ËΩΩÊ®°Âûã\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "ROI_EMBEDDING_DIM = 32\n",
    "NUM_ROI_CLASSES = 726 + 1\n",
    "HIDDEN_DIM = 64\n",
    "model=RESNET152_ATT_naive.resnet18(num_classes=NCLASS, input_ch=3+ROI_EMBEDDING_DIM)\n",
    "# init ROI Embedding layer\n",
    "roi_embedding_layer = nn.Embedding(NUM_ROI_CLASSES, ROI_EMBEDDING_DIM).to(device)\n",
    "# init FE\n",
    "roi_extractor = ROIFeatureExtractor(roi_embedding_layer, ROI_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM).to(device)\n",
    "roi_extractor.to(device)\n",
    "model.to(device)\n",
    "clustering_layer = ClusterlingLayer(embedding_dimension=512, num_clusters=NCLASS, alpha=1.0)\n",
    "kl_loss = KLDivLoss(NCLASS, loss_weight=2.0, temperature=2)\n",
    "kl_loss.to(device)\n",
    "clustering_layer.to(device)\n",
    "# 2Ô∏è‚É£ Âä†ËΩΩÊùÉÈáç\n",
    "state_dict = torch.load(modelpath, map_location=device)\n",
    "state_dict_FE = torch.load(fe_path, map_location=device)\n",
    "state_dict_cls = torch.load(cls_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "roi_extractor.load_state_dict(state_dict_FE)\n",
    "clustering_layer.load_state_dict(state_dict_cls)\n",
    "# 0.9115309895465665 0.919601178493508 0.913951033276079  \n",
    "# 0.9038265169218341 0.9223096121975994 0.9100564454400993\n",
    "model.eval()\n",
    "roi_extractor.eval()\n",
    "clustering_layer.eval()\n",
    "\n",
    "log_testing_total_loss = 0.0\n",
    "log_focal_loss = 0.0\n",
    "log_centering_loss= 0.\n",
    "log_clustering_loss = 0.0\n",
    "probs = []\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "global global_cluster_rois  # Ensure global access to cluster anatomical profiles\n",
    "loss_nll = nn.NLLLoss(size_average=True) # log-softmax applied in the network\n",
    "with torch.no_grad():\n",
    "    logit, attVec = test(model, roi_extractor, tst_loader, device)\n",
    "    attVec=mySoftmax(np.squeeze(np.vstack(attVec))).astype(np.float32)\n",
    "    print('size of attVec',attVec.shape)\n",
    "    #build output\n",
    "    prob=np.exp(np.vstack(logit)).astype(np.float32)\n",
    "    membership=np.argmax(prob,axis=1).reshape((-1,1)).astype(np.float32)\n",
    "    maxprob=np.amax(prob,axis=1).reshape((-1,1)).astype(np.float32)\n",
    "    \n",
    "    output_max=np.zeros((X_test_np.shape[0],7),dtype=np.float32)\n",
    "    output_max[:,0]=np.arange(1,X_test_np.shape[0]+1)\n",
    "    for i in range(X_test_np.shape[0]):\n",
    "        output_max[i, 0] = i + 1  # Fiber IDÔºà‰ªé 1 ÂºÄÂßãÔºâ\n",
    "        output_max[i, 1:4] = X_test_np[i, 0:3, 0]  # ÂèñÂâç‰∏â‰∏™ÈÄöÈÅìÁöÑÂùêÊ†á\n",
    "        output_max[i, 4] = X_test_np[i, 3, 0]  # Â≠òÂÇ® ROI ‰ø°ÊÅØ\n",
    "    #merge\n",
    "    output_max=np.hstack((output_max,prob,membership,maxprob))\n",
    "    np.savetxt(matpath.replace('.mat','.txt'),output_max,fmt='%.4e')\n",
    "        # Compute clustering loss if enabled\n",
    "    for i in range(NCLASS):\n",
    "        #print(i)\n",
    "        submat=output_max[np.where(output_max[:,-2]==i)]\n",
    "        #fiber index\n",
    "        fiberIndex=submat[:,0].reshape((-1,1))\n",
    "        np.savetxt(matpath.replace('.mat','_'+'{0:02}'.format(i)+'_fiberindex.txt'),fiberIndex,fmt='%d')\n",
    "        #fiber prob\n",
    "        fiberProb=submat[:,-1].reshape((-1,1))\n",
    "        np.savetxt(matpath.replace('.mat','_'+'{0:02}'.format(i)+'_fiberprob.txt'),fiberProb,fmt='%.4e')\n",
    "        #fiber attention map\n",
    "        fiberAtm=attVec[np.where(output_max[:,-2]==i)]\n",
    "        #print('fiberAtm.shape',fiberAtm.shape)\n",
    "        np.savetxt(matpath.replace('.mat','_'+'{0:02}'.format(i)+'_fiberatm.txt'),fiberAtm,fmt='%.4e')        \n",
    "    print('results saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_np shape: (469082, 4, 100)\n",
      "Sample data: [[-65.00016   -64.53611   -63.89756   -63.2625    -62.64133   -62.020386\n",
      "  -61.442703  -60.896885  -60.38636   -59.964584  -59.535625  -58.96403\n",
      "  -58.362305  -57.720272  -57.068596  -56.503193  -55.987263  -55.544903\n",
      "  -55.112076  -54.632645  -54.070496  -53.521496  -52.960003  -52.42095\n",
      "  -51.86682   -51.236607  -50.614723  -49.98778   -49.344894  -48.761734\n",
      "  -48.19622   -47.72383   -47.265675  -46.75627   -46.230648  -45.641064\n",
      "  -45.04124   -44.43267   -43.778633  -43.20743   -42.64826   -42.0742\n",
      "  -41.46937   -40.846848  -40.252407  -39.64069   -39.064342  -38.444927\n",
      "  -37.86092   -37.244854  -36.685444  -36.114853  -35.611057  -35.092754\n",
      "  -34.570095  -33.9988    -33.416508  -32.779728  -32.186512  -31.600033\n",
      "  -31.10708   -30.683212  -30.342094  -30.116259  -30.05608   -30.042112\n",
      "  -30.039797  -30.0403    -30.003862  -29.942793  -29.870632  -29.778378\n",
      "  -29.690678  -29.59237   -29.5686    -29.628315  -29.716938  -29.862934\n",
      "  -29.941267  -29.901352  -29.753572  -29.410868  -29.123024  -28.901161\n",
      "  -28.693308  -28.46276   -28.157333  -27.758356  -27.398674  -27.0124\n",
      "  -26.58169   -26.11781   -25.734016  -25.354046  -24.957836  -24.504557\n",
      "  -23.927526  -23.303854  -22.68994   -22.073053 ]\n",
      " [ -8.238065   -8.580183   -8.8432045  -8.958588   -9.08037    -9.291477\n",
      "   -9.45204    -9.549636   -9.538213   -9.561714   -9.618327   -9.722491\n",
      "   -9.763809   -9.761576   -9.747072   -9.78423    -9.891859  -10.108349\n",
      "  -10.401012  -10.742704  -10.911076  -11.009087  -11.078047  -11.195415\n",
      "  -11.328724  -11.479393  -11.610501  -11.753139  -11.940948  -12.140582\n",
      "  -12.35845   -12.556676  -12.749454  -12.9603405 -13.049337  -13.056522\n",
      "  -13.020525  -13.022592  -13.15476   -13.404909  -13.778038  -14.043396\n",
      "  -14.260012  -14.413137  -14.578808  -14.810324  -15.037966  -15.2884\n",
      "  -15.513169  -15.744109  -15.953544  -16.176174  -16.409868  -16.666279\n",
      "  -16.91908   -17.185207  -17.404074  -17.605753  -17.779652  -17.913683\n",
      "  -17.903522  -17.817133  -17.683447  -17.53351   -17.381615  -17.230461\n",
      "  -17.073675  -16.935324  -16.898388  -16.911665  -16.889     -16.844305\n",
      "  -16.81506   -16.79527   -16.801685  -16.841064  -16.87111   -16.887484\n",
      "  -16.900515  -16.92148   -16.9102    -16.833618  -16.754072  -16.669834\n",
      "  -16.596594  -16.525682  -16.492758  -16.493254  -16.545738  -16.653032\n",
      "  -16.836157  -17.083649  -17.386856  -17.709454  -17.971918  -18.173738\n",
      "  -18.248573  -18.241375  -18.114214  -17.94241  ]\n",
      " [ 16.675165   16.793867   16.945707   17.008297   17.078043   17.206673\n",
      "   17.42932    17.742977   18.175137   18.65538    19.128855   19.457705\n",
      "   19.638859   19.750326   19.892792   20.161436   20.546204   20.99239\n",
      "   21.348288   21.640076   21.924244   22.225294   22.573952   22.919245\n",
      "   23.191216   23.355795   23.40482    23.415377   23.440792   23.567942\n",
      "   23.815664   24.219835   24.620846   25.00289    25.332167   25.642715\n",
      "   25.890715   26.064068   26.118364   26.14348    26.160908   26.220228\n",
      "   26.314604   26.458878   26.603775   26.751947   26.847668   26.904442\n",
      "   26.9661     27.059477   27.257385   27.527239   27.834015   28.169498\n",
      "   28.430708   28.656418   28.777506   28.807789   28.673594   28.396498\n",
      "   27.99665    27.507551   26.969078   26.389297   25.757341   25.113157\n",
      "   24.484318   23.836231   23.208824   22.541155   21.921213   21.25586\n",
      "   20.639437   19.971409   19.35232    18.683422   18.062021   17.400259\n",
      "   16.779535   16.104458   15.502725   14.93447    14.383096   13.754068\n",
      "   13.165487   12.536086   11.99419    11.454729   10.937906   10.406352\n",
      "    9.98093     9.574791    9.1641655   8.738934    8.293832    7.87914\n",
      "    7.6011524   7.38561     7.236625    7.115001 ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.        145.          0.          0.          0.\n",
      "    0.          0.          0.        160.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        726.\n",
      "  726.        726.        726.        726.        726.        726.\n",
      "  726.        726.        726.        726.        726.        726.\n",
      "  726.        726.        726.        726.        726.        726.\n",
      "  726.        726.        726.        726.       ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test_np shape:\", X_test_np.shape)\n",
    "print(\"Sample data:\", X_test_np[0])  # ÊâìÂç∞Á¨¨‰∏Ä‰∏™Ê†∑Êú¨\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deterministic-a-bridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
