{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import h5py\n",
    "import RESNET152_ATT_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmat(filename):\n",
    "    '''\n",
    "    è¯»å– MATLAB v7.3 `.mat` æ–‡ä»¶ï¼ˆWhole_tracks ä½œä¸º tracksï¼‰\n",
    "    '''\n",
    "    output = dict()\n",
    "    \n",
    "    # æ‰“å¼€ HDF5 MAT æ–‡ä»¶\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        # è¯»å– Whole_tracks å˜é‡\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"âŒ é”™è¯¯: 'Whole_tracks' å˜é‡ä¸å­˜åœ¨ï¼\")\n",
    "\n",
    "        whole_tracks = data['Whole_tracks']  # ç»“æ„ä½“ Whole_tracks\n",
    "\n",
    "        # ç¡®ä¿å®ƒæœ‰ `count` å’Œ `data`\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"âŒ é”™è¯¯: 'Whole_tracks' ç»“æ„ä¸å®Œæ•´ï¼åŒ…å«: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # è¯»å– countï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ç¼–ç æ ¼å¼ï¼Œéœ€è¦è§£æï¼‰\n",
    "        count = whole_tracks['count'][()]  \n",
    "        print(\"ğŸ” Whole_tracks['count'] æ•°æ®:\", count)\n",
    "        print(\"ğŸ” æ•°æ®ç±»å‹:\", type(count))\n",
    "\n",
    "        # ç›´æ¥è½¬æ¢æˆæ•´æ•°\n",
    "        total_count = int(count.item())\n",
    "        print(f'total_count: {total_count}')\n",
    "        # è¯»å– Whole_tracks['data']\n",
    "        track = []\n",
    "        for i in range(total_count):\n",
    "            data_ref = whole_tracks['data'][i].item()\n",
    "            track.append(np.transpose(data[data_ref][:]).astype(np.float32))\n",
    "\n",
    "        # ç»„ç»‡è¾“å‡º\n",
    "        output['tracks'] = {\n",
    "            'count': total_count,\n",
    "            'data': track\n",
    "        }\n",
    "    \n",
    "    return output\n",
    "\n",
    "def _check_keys(dict):\n",
    "    '''\n",
    "    checks if entries in dictionary are mat-objects. If yes\n",
    "    todict is called to change them to nested dictionaries\n",
    "    '''\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict        \n",
    "\n",
    "def _todict(matobj):\n",
    "    '''\n",
    "    A recursive function which constructs from matobjects nested dictionaries\n",
    "    '''\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict\n",
    "\n",
    "#%%\n",
    "def mySoftmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\"\"\"normalize\"\"\"#110\n",
    "def rescale(X_list,count):\n",
    "    output=list()\n",
    "    if count==1:\n",
    "        output.append(X_list/110)\n",
    "        return output\n",
    "    for i in range(len(X_list)):\n",
    "        output.append(X_list[i]/110)\n",
    "    return output\n",
    "\n",
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "def datato3d(arrays):#list of np arrays, NULL*3*100\n",
    "    output=list()\n",
    "    for i in arrays:\n",
    "        i=np.squeeze(i,axis=1)\n",
    "        i=np.transpose(i,(0,2,1))\n",
    "        output.append(i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ å¤„ç†æ•°æ®: ../Testing_Set/J0037_tracks.mat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.utils.data as utils\n",
    "\n",
    "# å‚æ•°\n",
    "matpath = '../Testing_Set/J0037_tracks.mat'\n",
    "label_path = '../Testing_Set/J0037_class_label.mat'  # ä½ çš„æ ‡ç­¾æ–‡ä»¶\n",
    "classnum = 15  # ç±»åˆ«æ•°\n",
    "ROI_EMBEDDING_DIM = 32\n",
    "\n",
    "def loadmat(filename):\n",
    "    \"\"\" è¯»å– MATLAB v7.3 .mat æ–‡ä»¶ \"\"\"\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"âŒ é”™è¯¯: 'Whole_tracks' å˜é‡ä¸å­˜åœ¨ï¼\")\n",
    "        \n",
    "        whole_tracks = data['Whole_tracks']\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"âŒ é”™è¯¯: 'Whole_tracks' ç»“æ„ä¸å®Œæ•´ï¼åŒ…å«: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # è¯»å– count\n",
    "        count = int(whole_tracks['count'][()].item())\n",
    "        track = [np.transpose(data[whole_tracks['data'][i].item()][:]).astype(np.float32) for i in range(count)]\n",
    "    \n",
    "    return {'tracks': {'count': count, 'data': track}}\n",
    "\n",
    "def load_labels(label_path):\n",
    "    \"\"\" è¯»å–æ ‡ç­¾ .mat æ–‡ä»¶ \"\"\"\n",
    "    with h5py.File(label_path, 'r') as data:\n",
    "        if 'class_label' not in data:\n",
    "            raise KeyError(\"âŒ é”™è¯¯: 'class_label' å˜é‡ä¸å­˜åœ¨ï¼\")\n",
    "        \n",
    "        class_label = data['class_label'][()]\n",
    "        \n",
    "        if isinstance(class_label, np.ndarray):\n",
    "            if class_label.size == 1:  \n",
    "                class_label = class_label.item()\n",
    "            else:  \n",
    "                class_label = np.array(class_label)\n",
    "        else:\n",
    "            class_label = int(class_label)\n",
    "\n",
    "        print(f\"âœ… æˆåŠŸè§£æ class_label, å½¢çŠ¶: {class_label.shape}\")\n",
    "        return class_label\n",
    "\n",
    "\n",
    "\"\"\" æµ‹è¯•æ¨¡å‹ \"\"\"\n",
    "args_test_batch_size = 10000\n",
    "NCLASS = int(classnum)\n",
    "\n",
    "print(f\"ğŸ“Œ å¤„ç†æ•°æ®: {matpath}\")\n",
    "mat = loadmat(matpath)\n",
    "X_test = mat['tracks']['data']\n",
    "X_test = np.asarray(X_test).astype(np.float32)\n",
    "X_test_original = np.transpose(X_test, (0, 2, 1))  # ç»´åº¦è½¬æ¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸè§£æ class_label, å½¢çŠ¶: (234541, 1)\n",
      "(234541, 4, 100)\n",
      "(234541, 1)\n",
      "(469082, 4, 100)\n",
      "(469082,)\n"
     ]
    }
   ],
   "source": [
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "    y_nparray = y_nparray.flatten()\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "# è¯»å–æ ‡ç­¾\n",
    "y_test = load_labels(label_path)\n",
    "y_test_list = y_test\n",
    "print(X_test_original.shape)\n",
    "print(y_test.shape)\n",
    "X_test, y_test = udflip(X_test_original,y_test,shuffle=False)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "y_test = torch.from_numpy(y_test.astype(np.int64))  # ç¡®ä¿æ˜¯æ•´æ•°ç±»å‹\n",
    "X_test = torch.from_numpy(X_test)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "tst_set = utils.TensorDataset(X_test, y_test)\n",
    "tst_loader = utils.DataLoader(tst_set, batch_size=args_test_batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469082"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([469082, 4, 100]), torch.Size([469082]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_at_test(probs,mode='max'):\n",
    "    assert(len(probs)>0)\n",
    "    if(mode=='max'):\n",
    "        all_probs=np.vstack(probs)\n",
    "        print(all_probs.shape)\n",
    "        max_probs=np.amax(all_probs,axis=1).reshape((2,-1))#row 0: prob for first half, row 1: prob for flipped half\n",
    "        max_idx=np.argmax(max_probs,axis=0)#should be 0/1\n",
    "        test_sample_count=all_probs.shape[0]/2\n",
    "        \n",
    "        class_pred=np.argmax(all_probs,axis=1)\n",
    "        final_pred=list()\n",
    "        for i in range(max_idx.shape[0]):\n",
    "            final_pred.append(class_pred[int(i+test_sample_count*max_idx[i])])#if 0, first half\n",
    "        return final_pred\n",
    "    if(mode=='mean'):\n",
    "        all_probs=np.exp(np.vstack(probs))\n",
    "        test_sample_count=int(all_probs.shape[0]/2)\n",
    "        final_probs=all_probs[0:test_sample_count]+all_probs[test_sample_count:]\n",
    "        final_pred=np.argmax(final_probs,axis=1)\n",
    "        return final_pred.tolist()\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def aug_at_test(probs, mode='max'):\n",
    "#     \"\"\"\n",
    "#     é€‚ç”¨äº **æ— æ•°æ®å¢å¼º** çš„ç‰ˆæœ¬ï¼š\n",
    "#     - ç›´æ¥é€‰æ‹© `argmax` ä½œä¸ºæœ€ç»ˆé¢„æµ‹\n",
    "#     - `mode='max'` æˆ– `mode='mean'` å½±å“ä¸å¤§ï¼Œå› ä¸ºæ²¡æœ‰ç¿»è½¬æ•°æ®\n",
    "    \n",
    "#     å‚æ•°ï¼š\n",
    "#     - probs: æ¨¡å‹è¾“å‡ºçš„ logits åˆ—è¡¨ï¼Œæ¯ä¸ª batch å­˜å‚¨ä¸€æ¬¡è¾“å‡º\n",
    "    \n",
    "#     è¿”å›ï¼š\n",
    "#     - final_pred: é¢„æµ‹ç±»åˆ«åˆ—è¡¨\n",
    "#     \"\"\"\n",
    "#     assert len(probs) > 0, \"probs ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—é¢„æµ‹ç»“æœ\"\n",
    "\n",
    "#     # åˆå¹¶æ‰€æœ‰ batch\n",
    "#     all_probs = np.vstack(probs)  # å½¢çŠ¶: (N, num_classes)\n",
    "\n",
    "#     # ç›´æ¥å–æœ€å¤§æ¦‚ç‡ç±»åˆ«ä½œä¸ºé¢„æµ‹ç±»åˆ«\n",
    "#     final_pred = np.argmax(all_probs, axis=1)\n",
    "\n",
    "#     return final_pred.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:1\n",
      "using ROI with emb: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bohan/.conda/envs/deterministic-a-bridge/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/bohan/projects/DCNN_Dice/RESNET152_ATT_naive.py:212: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x), embed, x_att, x, out1, out2, out3, final_feat, out1_feat, out2_feat, out3_feat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(469082, 15)\n",
      "\tCenter loss: 0.0000\n",
      "\tfocal loss: 0.0293\n",
      "Test set avg loss: 362.4790\n",
      "\tClustering loss: 362.4497\n",
      "Precision, Recall, macro F1: 0.9139219235052434 0.9907362324880048 0.9503685670964716\n"
     ]
    }
   ],
   "source": [
    "# æ•°æ®åŠ è½½\n",
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "from Embedding_layer import ROIFeatureExtractor\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import RESNET152_ATT_naive\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "from Util import focalLoss, preprocess_fiber_input\n",
    "from clustering_layer_v2 import ClusterlingLayer\n",
    "from klDiv import KLDivLoss\n",
    "\n",
    "modelpath = 'focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "fe_path = 'FE_layer_focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "cls_path = 'CLS_layer_focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "# åŠ è½½æ¨¡å‹\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "ROI_EMBEDDING_DIM = 32\n",
    "NUM_ROI_CLASSES = 726 + 1\n",
    "HIDDEN_DIM = 64\n",
    "model=RESNET152_ATT_naive.resnet18(num_classes=NCLASS, input_ch=3+ROI_EMBEDDING_DIM)\n",
    "# init ROI Embedding layer\n",
    "roi_embedding_layer = nn.Embedding(NUM_ROI_CLASSES, ROI_EMBEDDING_DIM).to(device)\n",
    "# init FE\n",
    "roi_extractor = ROIFeatureExtractor(roi_embedding_layer, ROI_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM).to(device)\n",
    "roi_extractor.to(device)\n",
    "model.to(device)\n",
    "clustering_layer = ClusterlingLayer(embedding_dimension=512, num_clusters=NCLASS, alpha=1.0)\n",
    "kl_loss = KLDivLoss(NCLASS, loss_weight=2.0, temperature=2)\n",
    "kl_loss.to(device)\n",
    "clustering_layer.to(device)\n",
    "# 2ï¸âƒ£ åŠ è½½æƒé‡\n",
    "state_dict = torch.load(modelpath, map_location=device)\n",
    "state_dict_FE = torch.load(fe_path, map_location=device)\n",
    "state_dict_cls = torch.load(cls_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "roi_extractor.load_state_dict(state_dict_FE)\n",
    "clustering_layer.load_state_dict(state_dict_cls)\n",
    "# 0.9115309895465665 0.919601178493508 0.913951033276079  \n",
    "# 0.9038265169218341 0.9223096121975994 0.9100564454400993\n",
    "model.eval()\n",
    "roi_extractor.eval()\n",
    "clustering_layer.eval()\n",
    "\n",
    "log_testing_total_loss = 0.0\n",
    "log_focal_loss = 0.0\n",
    "log_centering_loss= 0.\n",
    "log_clustering_loss = 0.0\n",
    "probs = []\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "global global_cluster_rois  # Ensure global access to cluster anatomical profiles\n",
    "loss_nll = nn.NLLLoss(size_average=True) # log-softmax applied in the network\n",
    "with torch.no_grad():\n",
    "    for data, target in tst_loader:\n",
    "        labels += target.cpu().numpy().tolist()\n",
    "        # if args.cuda:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # target = target.squeeze(1)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # print(data.shape)\n",
    "        # print(target.shape)\n",
    "        data_processed = preprocess_fiber_input(data, roi_extractor=roi_extractor, device=device, net_type='FE')\n",
    "        output, embed, _, _, _, _, _, _, _, _, _ = model(data_processed)\n",
    "\n",
    "        # Compute focal loss\n",
    "        floss = focalLoss(output, target, loss_nll=loss_nll)\n",
    "        total_loss = floss\n",
    "        log_focal_loss += floss.item()\n",
    "        # Compute center loss if enabled\n",
    "\n",
    "        # Compute clustering loss if enabled\n",
    "        clustering_out, x_dis = clustering_layer(embed)\n",
    "\n",
    "        # Get predicted cluster labels\n",
    "        tar_dist = ClusterlingLayer.create_soft_labels(target, NCLASS, temperature=2).to(target.device)\n",
    "        loss_clust = 10 * kl_loss.kl_div_cluster(torch.log(clustering_out), tar_dist) / 1024\n",
    "\n",
    "        total_loss += loss_clust\n",
    "        log_clustering_loss += loss_clust.item()\n",
    "\n",
    "        # Accumulate total test loss\n",
    "        log_testing_total_loss += total_loss.item()\n",
    "        probs.append(output.data.cpu().numpy())\n",
    "\n",
    "# Compute final predictions using test-time augmentation\n",
    "preds = aug_at_test(probs, mode='max')\n",
    "num_batch = len(tst_loader) / 10000\n",
    "\n",
    "# Compute evaluation metrics\n",
    "conf_mat = confusion_matrix(y_test_list, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_list, preds, average='macro')\n",
    "\n",
    "avg_testing_loss = log_testing_total_loss / num_batch\n",
    "avg_clustering_loss = log_clustering_loss / num_batch\n",
    "print('\\tCenter loss: {:.4f}'.format(log_centering_loss / num_batch))\n",
    "print('\\tfocal loss: {:.4f}'.format(log_focal_loss/num_batch))\n",
    "print(f'Test set avg loss: {avg_testing_loss:.4f}')\n",
    "\n",
    "print(f'\\tClustering loss: {avg_clustering_loss:.4f}')\n",
    "print('Precision, Recall, macro F1:', precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.9996\n",
      "AUPRC: 0.9860\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# è¿‡æ»¤ None å€¼\n",
    "probs = [p for p in probs if p is not None]\n",
    "\n",
    "# ç”Ÿæˆå¯¹åº”çš„æ ‡ç­¾ (ground truth)\n",
    "# ç¡®ä¿ probs æ˜¯ NumPy æ•°ç»„\n",
    "try:\n",
    "    probs = np.concatenate(probs, axis=0)  # (num_samples, num_classes)\n",
    "except ValueError as e:\n",
    "    print(f\"probs å½¢çŠ¶ä¸åŒ¹é…: {e}\")\n",
    "    for i, p in enumerate(probs):\n",
    "        print(f\"probs[{i}] å½¢çŠ¶: {p.shape}\")  # æ£€æŸ¥å“ªä¸ªå…ƒç´ æœ‰é—®é¢˜\n",
    "    probs = np.vstack([p for p in probs if p.shape == probs[0].shape])  # åªä¿ç•™å½¢çŠ¶åŒ¹é…çš„\n",
    "\n",
    "# åº”ç”¨ softmax ç¡®ä¿æ˜¯æ¦‚ç‡åˆ†å¸ƒ\n",
    "probs = F.softmax(torch.tensor(probs), dim=1).numpy()\n",
    "\n",
    "# è®¡ç®— AUROC å’Œ AUPRC\n",
    "try:\n",
    "    labels = np.array(labels)  # ç¡®ä¿ labels æ˜¯ numpy æ•°ç»„\n",
    "    preds = np.argmax(probs, axis=1)  # è®¡ç®—é¢„æµ‹ç±»åˆ«\n",
    "    \n",
    "    auroc = roc_auc_score(labels, probs, multi_class='ovr')\n",
    "    auprc = average_precision_score(labels, probs, average='macro')\n",
    "except ValueError as e:\n",
    "    print(f\"AUROC / AUPRC è®¡ç®—é”™è¯¯: {e}\")\n",
    "    auroc, auprc = None, None\n",
    "\n",
    "# è¾“å‡º\n",
    "if auroc is not None and auprc is not None:\n",
    "    print(f\"AUROC: {auroc:.4f}\")\n",
    "    print(f\"AUPRC: {auprc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deterministic-a-bridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
