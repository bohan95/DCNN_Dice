{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import h5py\n",
    "import RESNET152_ATT_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmat(filename):\n",
    "    '''\n",
    "    读取 MATLAB v7.3 `.mat` 文件（Whole_tracks 作为 tracks）\n",
    "    '''\n",
    "    output = dict()\n",
    "    \n",
    "    # 打开 HDF5 MAT 文件\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        # 读取 Whole_tracks 变量\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'Whole_tracks' 变量不存在！\")\n",
    "\n",
    "        whole_tracks = data['Whole_tracks']  # 结构体 Whole_tracks\n",
    "\n",
    "        # 确保它有 `count` 和 `data`\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"❌ 错误: 'Whole_tracks' 结构不完整！包含: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # 读取 count（可能是字符编码格式，需要解析）\n",
    "        count = whole_tracks['count'][()]  \n",
    "        print(\"🔍 Whole_tracks['count'] 数据:\", count)\n",
    "        print(\"🔍 数据类型:\", type(count))\n",
    "\n",
    "        # 直接转换成整数\n",
    "        total_count = int(count.item())\n",
    "        print(f'total_count: {total_count}')\n",
    "        # 读取 Whole_tracks['data']\n",
    "        track = []\n",
    "        for i in range(total_count):\n",
    "            data_ref = whole_tracks['data'][i].item()\n",
    "            track.append(np.transpose(data[data_ref][:]).astype(np.float32))\n",
    "\n",
    "        # 组织输出\n",
    "        output['tracks'] = {\n",
    "            'count': total_count,\n",
    "            'data': track\n",
    "        }\n",
    "    \n",
    "    return output\n",
    "\n",
    "def _check_keys(dict):\n",
    "    '''\n",
    "    checks if entries in dictionary are mat-objects. If yes\n",
    "    todict is called to change them to nested dictionaries\n",
    "    '''\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict        \n",
    "\n",
    "def _todict(matobj):\n",
    "    '''\n",
    "    A recursive function which constructs from matobjects nested dictionaries\n",
    "    '''\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict\n",
    "\n",
    "#%%\n",
    "def mySoftmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\"\"\"normalize\"\"\"#110\n",
    "def rescale(X_list,count):\n",
    "    output=list()\n",
    "    if count==1:\n",
    "        output.append(X_list/110)\n",
    "        return output\n",
    "    for i in range(len(X_list)):\n",
    "        output.append(X_list[i]/110)\n",
    "    return output\n",
    "\n",
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "def datato3d(arrays):#list of np arrays, NULL*3*100\n",
    "    output=list()\n",
    "    for i in arrays:\n",
    "        i=np.squeeze(i,axis=1)\n",
    "        i=np.transpose(i,(0,2,1))\n",
    "        output.append(i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 处理数据: ../Testing_Set/J0037_tracks.mat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.utils.data as utils\n",
    "\n",
    "# 参数\n",
    "matpath = '../Testing_Set/J0037_tracks.mat'\n",
    "label_path = '../Testing_Set/J0037_class_label.mat'  # 你的标签文件\n",
    "classnum = 15  # 类别数\n",
    "ROI_EMBEDDING_DIM = 32\n",
    "\n",
    "def loadmat(filename):\n",
    "    \"\"\" 读取 MATLAB v7.3 .mat 文件 \"\"\"\n",
    "    with h5py.File(filename, 'r') as data:\n",
    "        if 'Whole_tracks' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'Whole_tracks' 变量不存在！\")\n",
    "        \n",
    "        whole_tracks = data['Whole_tracks']\n",
    "        if 'count' not in whole_tracks or 'data' not in whole_tracks:\n",
    "            raise KeyError(f\"❌ 错误: 'Whole_tracks' 结构不完整！包含: {list(whole_tracks.keys())}\")\n",
    "\n",
    "        # 读取 count\n",
    "        count = int(whole_tracks['count'][()].item())\n",
    "        track = [np.transpose(data[whole_tracks['data'][i].item()][:]).astype(np.float32) for i in range(count)]\n",
    "    \n",
    "    return {'tracks': {'count': count, 'data': track}}\n",
    "\n",
    "def load_labels(label_path):\n",
    "    \"\"\" 读取标签 .mat 文件 \"\"\"\n",
    "    with h5py.File(label_path, 'r') as data:\n",
    "        if 'class_label' not in data:\n",
    "            raise KeyError(\"❌ 错误: 'class_label' 变量不存在！\")\n",
    "        \n",
    "        class_label = data['class_label'][()]\n",
    "        \n",
    "        if isinstance(class_label, np.ndarray):\n",
    "            if class_label.size == 1:  \n",
    "                class_label = class_label.item()\n",
    "            else:  \n",
    "                class_label = np.array(class_label)\n",
    "        else:\n",
    "            class_label = int(class_label)\n",
    "\n",
    "        print(f\"✅ 成功解析 class_label, 形状: {class_label.shape}\")\n",
    "        return class_label\n",
    "\n",
    "\n",
    "\"\"\" 测试模型 \"\"\"\n",
    "args_test_batch_size = 10000\n",
    "NCLASS = int(classnum)\n",
    "\n",
    "print(f\"📌 处理数据: {matpath}\")\n",
    "mat = loadmat(matpath)\n",
    "X_test = mat['tracks']['data']\n",
    "X_test = np.asarray(X_test).astype(np.float32)\n",
    "X_test_original = np.transpose(X_test, (0, 2, 1))  # 维度转换\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功解析 class_label, 形状: (234541, 1)\n",
      "(234541, 4, 100)\n",
      "(234541, 1)\n",
      "(469082, 4, 100)\n",
      "(469082,)\n"
     ]
    }
   ],
   "source": [
    "def udflip(X_nparray, y_nparray, shuffle=True):\n",
    "\n",
    "    if X_nparray.shape[2] == 4:\n",
    "        if np.std(X_nparray[:, 0, :]) > np.std(X_nparray[:, -1, :]):\n",
    "            print(\"Detected special info in first column, swapping...\")\n",
    "            X_nparray = np.concatenate((X_nparray[:, 1:, :], X_nparray[:, 0:1, :]), axis=1)\n",
    "    \n",
    "    X_flipped = np.flip(X_nparray, axis=2)  \n",
    "    y_nparray = y_nparray.flatten()\n",
    "    X_aug = np.vstack((X_nparray, X_flipped))\n",
    "    y_aug = np.hstack((y_nparray, y_nparray))  \n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_idx = np.random.permutation(X_aug.shape[0])\n",
    "        return X_aug[shuffle_idx], y_aug[shuffle_idx]\n",
    "    else:\n",
    "        return X_aug, y_aug\n",
    "# 读取标签\n",
    "y_test = load_labels(label_path)\n",
    "y_test_list = y_test\n",
    "print(X_test_original.shape)\n",
    "print(y_test.shape)\n",
    "X_test, y_test = udflip(X_test_original,y_test,shuffle=False)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "y_test = torch.from_numpy(y_test.astype(np.int64))  # 确保是整数类型\n",
    "X_test = torch.from_numpy(X_test)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "tst_set = utils.TensorDataset(X_test, y_test)\n",
    "tst_loader = utils.DataLoader(tst_set, batch_size=args_test_batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469082"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([469082, 4, 100]), torch.Size([469082]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_at_test(probs,mode='max'):\n",
    "    assert(len(probs)>0)\n",
    "    if(mode=='max'):\n",
    "        all_probs=np.vstack(probs)\n",
    "        print(all_probs.shape)\n",
    "        max_probs=np.amax(all_probs,axis=1).reshape((2,-1))#row 0: prob for first half, row 1: prob for flipped half\n",
    "        max_idx=np.argmax(max_probs,axis=0)#should be 0/1\n",
    "        test_sample_count=all_probs.shape[0]/2\n",
    "        \n",
    "        class_pred=np.argmax(all_probs,axis=1)\n",
    "        final_pred=list()\n",
    "        for i in range(max_idx.shape[0]):\n",
    "            final_pred.append(class_pred[int(i+test_sample_count*max_idx[i])])#if 0, first half\n",
    "        return final_pred\n",
    "    if(mode=='mean'):\n",
    "        all_probs=np.exp(np.vstack(probs))\n",
    "        test_sample_count=int(all_probs.shape[0]/2)\n",
    "        final_probs=all_probs[0:test_sample_count]+all_probs[test_sample_count:]\n",
    "        final_pred=np.argmax(final_probs,axis=1)\n",
    "        return final_pred.tolist()\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def aug_at_test(probs, mode='max'):\n",
    "#     \"\"\"\n",
    "#     适用于 **无数据增强** 的版本：\n",
    "#     - 直接选择 `argmax` 作为最终预测\n",
    "#     - `mode='max'` 或 `mode='mean'` 影响不大，因为没有翻转数据\n",
    "    \n",
    "#     参数：\n",
    "#     - probs: 模型输出的 logits 列表，每个 batch 存储一次输出\n",
    "    \n",
    "#     返回：\n",
    "#     - final_pred: 预测类别列表\n",
    "#     \"\"\"\n",
    "#     assert len(probs) > 0, \"probs 为空，无法计算预测结果\"\n",
    "\n",
    "#     # 合并所有 batch\n",
    "#     all_probs = np.vstack(probs)  # 形状: (N, num_classes)\n",
    "\n",
    "#     # 直接取最大概率类别作为预测类别\n",
    "#     final_pred = np.argmax(all_probs, axis=1)\n",
    "\n",
    "#     return final_pred.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:1\n",
      "using ROI with emb: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bohan/.conda/envs/deterministic-a-bridge/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/bohan/projects/DCNN_Dice/RESNET152_ATT_naive.py:212: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x), embed, x_att, x, out1, out2, out3, final_feat, out1_feat, out2_feat, out3_feat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(469082, 15)\n",
      "\tCenter loss: 0.0000\n",
      "\tfocal loss: 0.0293\n",
      "Test set avg loss: 362.4790\n",
      "\tClustering loss: 362.4497\n",
      "Precision, Recall, macro F1: 0.9139219235052434 0.9907362324880048 0.9503685670964716\n"
     ]
    }
   ],
   "source": [
    "# 数据加载\n",
    "import os\n",
    "import sys\n",
    "# del os.environ['MKL_NUM_THREADS'] # error corrected by MH 10/12/2022 (add these three lines)\n",
    "from Embedding_layer import ROIFeatureExtractor\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import scipy.io as spio\n",
    "import RESNET152_ATT_naive\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "from Util import focalLoss, preprocess_fiber_input\n",
    "from clustering_layer_v2 import ClusterlingLayer\n",
    "from klDiv import KLDivLoss\n",
    "\n",
    "modelpath = 'focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "fe_path = 'FE_layer_focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "cls_path = 'CLS_layer_focal_loss_and_cluster_loss_c_10.0_FE_dim_32.model'\n",
    "# 加载模型\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "ROI_EMBEDDING_DIM = 32\n",
    "NUM_ROI_CLASSES = 726 + 1\n",
    "HIDDEN_DIM = 64\n",
    "model=RESNET152_ATT_naive.resnet18(num_classes=NCLASS, input_ch=3+ROI_EMBEDDING_DIM)\n",
    "# init ROI Embedding layer\n",
    "roi_embedding_layer = nn.Embedding(NUM_ROI_CLASSES, ROI_EMBEDDING_DIM).to(device)\n",
    "# init FE\n",
    "roi_extractor = ROIFeatureExtractor(roi_embedding_layer, ROI_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM).to(device)\n",
    "roi_extractor.to(device)\n",
    "model.to(device)\n",
    "clustering_layer = ClusterlingLayer(embedding_dimension=512, num_clusters=NCLASS, alpha=1.0)\n",
    "kl_loss = KLDivLoss(NCLASS, loss_weight=2.0, temperature=2)\n",
    "kl_loss.to(device)\n",
    "clustering_layer.to(device)\n",
    "# 2️⃣ 加载权重\n",
    "state_dict = torch.load(modelpath, map_location=device)\n",
    "state_dict_FE = torch.load(fe_path, map_location=device)\n",
    "state_dict_cls = torch.load(cls_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "roi_extractor.load_state_dict(state_dict_FE)\n",
    "clustering_layer.load_state_dict(state_dict_cls)\n",
    "# 0.9115309895465665 0.919601178493508 0.913951033276079  \n",
    "# 0.9038265169218341 0.9223096121975994 0.9100564454400993\n",
    "model.eval()\n",
    "roi_extractor.eval()\n",
    "clustering_layer.eval()\n",
    "\n",
    "log_testing_total_loss = 0.0\n",
    "log_focal_loss = 0.0\n",
    "log_centering_loss= 0.\n",
    "log_clustering_loss = 0.0\n",
    "probs = []\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "global global_cluster_rois  # Ensure global access to cluster anatomical profiles\n",
    "loss_nll = nn.NLLLoss(size_average=True) # log-softmax applied in the network\n",
    "with torch.no_grad():\n",
    "    for data, target in tst_loader:\n",
    "        labels += target.cpu().numpy().tolist()\n",
    "        # if args.cuda:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # target = target.squeeze(1)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # print(data.shape)\n",
    "        # print(target.shape)\n",
    "        data_processed = preprocess_fiber_input(data, roi_extractor=roi_extractor, device=device, net_type='FE')\n",
    "        output, embed, _, _, _, _, _, _, _, _, _ = model(data_processed)\n",
    "\n",
    "        # Compute focal loss\n",
    "        floss = focalLoss(output, target, loss_nll=loss_nll)\n",
    "        total_loss = floss\n",
    "        log_focal_loss += floss.item()\n",
    "        # Compute center loss if enabled\n",
    "\n",
    "        # Compute clustering loss if enabled\n",
    "        clustering_out, x_dis = clustering_layer(embed)\n",
    "\n",
    "        # Get predicted cluster labels\n",
    "        tar_dist = ClusterlingLayer.create_soft_labels(target, NCLASS, temperature=2).to(target.device)\n",
    "        loss_clust = 10 * kl_loss.kl_div_cluster(torch.log(clustering_out), tar_dist) / 1024\n",
    "\n",
    "        total_loss += loss_clust\n",
    "        log_clustering_loss += loss_clust.item()\n",
    "\n",
    "        # Accumulate total test loss\n",
    "        log_testing_total_loss += total_loss.item()\n",
    "        probs.append(output.data.cpu().numpy())\n",
    "\n",
    "# Compute final predictions using test-time augmentation\n",
    "preds = aug_at_test(probs, mode='max')\n",
    "num_batch = len(tst_loader) / 10000\n",
    "\n",
    "# Compute evaluation metrics\n",
    "conf_mat = confusion_matrix(y_test_list, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_list, preds, average='macro')\n",
    "\n",
    "avg_testing_loss = log_testing_total_loss / num_batch\n",
    "avg_clustering_loss = log_clustering_loss / num_batch\n",
    "print('\\tCenter loss: {:.4f}'.format(log_centering_loss / num_batch))\n",
    "print('\\tfocal loss: {:.4f}'.format(log_focal_loss/num_batch))\n",
    "print(f'Test set avg loss: {avg_testing_loss:.4f}')\n",
    "\n",
    "print(f'\\tClustering loss: {avg_clustering_loss:.4f}')\n",
    "print('Precision, Recall, macro F1:', precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.9996\n",
      "AUPRC: 0.9860\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 过滤 None 值\n",
    "probs = [p for p in probs if p is not None]\n",
    "\n",
    "# 生成对应的标签 (ground truth)\n",
    "# 确保 probs 是 NumPy 数组\n",
    "try:\n",
    "    probs = np.concatenate(probs, axis=0)  # (num_samples, num_classes)\n",
    "except ValueError as e:\n",
    "    print(f\"probs 形状不匹配: {e}\")\n",
    "    for i, p in enumerate(probs):\n",
    "        print(f\"probs[{i}] 形状: {p.shape}\")  # 检查哪个元素有问题\n",
    "    probs = np.vstack([p for p in probs if p.shape == probs[0].shape])  # 只保留形状匹配的\n",
    "\n",
    "# 应用 softmax 确保是概率分布\n",
    "probs = F.softmax(torch.tensor(probs), dim=1).numpy()\n",
    "\n",
    "# 计算 AUROC 和 AUPRC\n",
    "try:\n",
    "    labels = np.array(labels)  # 确保 labels 是 numpy 数组\n",
    "    preds = np.argmax(probs, axis=1)  # 计算预测类别\n",
    "    \n",
    "    auroc = roc_auc_score(labels, probs, multi_class='ovr')\n",
    "    auprc = average_precision_score(labels, probs, average='macro')\n",
    "except ValueError as e:\n",
    "    print(f\"AUROC / AUPRC 计算错误: {e}\")\n",
    "    auroc, auprc = None, None\n",
    "\n",
    "# 输出\n",
    "if auroc is not None and auprc is not None:\n",
    "    print(f\"AUROC: {auroc:.4f}\")\n",
    "    print(f\"AUPRC: {auprc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deterministic-a-bridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
